# 11.9. Real-World Case Studies

> Kinh nghiá»‡m thá»±c táº¿ tá»« production deployments

---

## ğŸ¯ Má»¥c TiÃªu

- âœ… Há»c tá»« real-world scenarios
- âœ… Architecture decisions
- âœ… Challenges vÃ  solutions
- âœ… Lessons learned

---

## ğŸ›’ Case Study 1: E-Commerce Platform

### Background

**Company:** Online retailer  
**Scale:** 10M users, 100K orders/day  
**Challenge:** Black Friday traffic (10x normal)  

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PRODUCTION CLUSTER              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  Frontend (React SPA)                   â”‚
â”‚  â”œâ”€ Deployment: 10-50 pods (HPA)       â”‚
â”‚  â”œâ”€ Ingress: NGINX + cert-manager      â”‚
â”‚  â””â”€ CDN: CloudFlare                    â”‚
â”‚                                         â”‚
â”‚  Backend Services (Microservices)       â”‚
â”‚  â”œâ”€ Order Service (10-30 pods)         â”‚
â”‚  â”œâ”€ Payment Service (5-20 pods)        â”‚
â”‚  â”œâ”€ Inventory Service (5-15 pods)      â”‚
â”‚  â””â”€ Notification Service (3-10 pods)   â”‚
â”‚                                         â”‚
â”‚  Databases                              â”‚
â”‚  â”œâ”€ PostgreSQL (StatefulSet)           â”‚
â”‚  â”œâ”€ Redis (cache)                      â”‚
â”‚  â””â”€ Elasticsearch (search)             â”‚
â”‚                                         â”‚
â”‚  Message Queue                          â”‚
â”‚  â””â”€ RabbitMQ (StatefulSet)             â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Decisions

**1. Microservices Architecture**
```
Why: Scalability, independent deployment
Challenge: Service discovery, distributed tracing
Solution: Istio service mesh
```

**2. Autoscaling Strategy**
```yaml
# HPA based on custom metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 10
  maxReplicas: 50
  metrics:
  - type: Pods
    pods:
      metric:
        name: orders_per_second
      target:
        type: AverageValue
        averageValue: "100"  # 100 orders/sec per pod
```

**3. Database Strategy**
```
Primary: PostgreSQL (StatefulSet)
Read replicas: 5 replicas
Cache: Redis (80% hit rate)
```

### Black Friday Preparation

**Pre-event:**
```bash
# 1. Pre-scale infrastructure
kubectl scale deployment frontend --replicas=30
kubectl scale deployment order-service --replicas=20

# 2. Warm up cache
# Run cache warming job

# 3. Load testing
# Simulate 10x traffic with k6

# 4. War room setup
# Incident channel, on-call rotation
```

**During Event:**
```
âœ“ Peak: 150K concurrent users
âœ“ Autoscaling worked perfectly
âœ“ 0 downtime
âœ“ P95 latency < 500ms
âœ“ 0 critical incidents
```

### Lessons Learned

**What Worked:**
```
âœ“ HPA with custom metrics
âœ“ Pre-scaling before event
âœ“ Extensive load testing
âœ“ Monitoring dashboards
âœ“ Runbooks for common issues
```

**What Didn't:**
```
âœ— Database connection pool exhausted (fixed: increased pool size)
âœ— Redis memory spike (fixed: added memory limits + eviction policy)
âœ— Some alerts too noisy (fixed: adjusted thresholds)
```

---

## ğŸ¢ Case Study 2: SaaS Platform Migration

### Background

**Company:** B2B SaaS  
**Migration:** Monolith â†’ Microservices on K8s  
**Timeline:** 6 months  
**Users:** 50K companies, 500K end users  

### Migration Strategy

**Phase 1: Strangler Pattern**
```
Monolith (VM)
    â†“
API Gateway (K8s)
    â”œâ”€ Route to Monolith (default)
    â””â”€ Route to new services (gradually)
```

**Phase 2: Service by Service**
```
Month 1-2: Auth Service â†’ K8s
Month 3-4: User Service â†’ K8s
Month 5-6: Core Business Logic â†’ K8s
```

### Architecture

```yaml
# Multi-tenant architecture
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-acme
  labels:
    tenant: acme
    tier: premium
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-quota
  namespace: tenant-acme
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    pods: "50"
```

### Challenges & Solutions

**Challenge 1: Data Migration**
```
Problem: 500GB database
Solution: 
  â€¢ Dual-write (old + new DB)
  â€¢ Background migration job
  â€¢ Verification scripts
  â€¢ Gradual cutover
```

**Challenge 2: Zero Downtime**
```
Solution:
  â€¢ Feature flags
  â€¢ Canary deployments
  â€¢ Instant rollback capability
  â€¢ Extensive monitoring
```

**Challenge 3: Multi-tenancy**
```
Solution:
  â€¢ Namespace per tenant
  â€¢ Resource quotas
  â€¢ Network policies
  â€¢ Tenant-specific configs
```

### Results

```
âœ“ Zero downtime during migration
âœ“ 40% cost reduction (vs VMs)
âœ“ 3x faster deployments
âœ“ 99.95% â†’ 99.99% uptime
âœ“ Better resource utilization
```

---

## ğŸ® Case Study 3: Gaming Backend

### Background

**Company:** Mobile game  
**Scale:** 5M DAU (Daily Active Users)  
**Challenge:** Spiky traffic, real-time gameplay  

### Architecture

```
Global Load Balancer
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Regional Clusters (3)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                               â”‚
â”‚  Game Servers (StatefulSet)   â”‚
â”‚  â”œâ”€ Sticky sessions           â”‚
â”‚  â”œâ”€ 1000 pods                 â”‚
â”‚  â””â”€ Autoscaling: 500-2000     â”‚
â”‚                               â”‚
â”‚  Matchmaking Service          â”‚
â”‚  â”œâ”€ Queue management          â”‚
â”‚  â””â”€ Low latency required      â”‚
â”‚                               â”‚
â”‚  Leaderboard Service          â”‚
â”‚  â”œâ”€ Redis Cluster             â”‚
â”‚  â””â”€ High read throughput      â”‚
â”‚                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Optimizations

**1. Pod Affinity (Sticky Sessions)**
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: game-server
spec:
  serviceName: game-server
  replicas: 1000
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - game-server
              topologyKey: kubernetes.io/hostname
```

**2. DaemonSet for Monitoring**
```yaml
# Low-overhead monitoring agent on every node
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: game-metrics
spec:
  template:
    spec:
      containers:
      - name: metrics
        image: game-metrics:v1
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
```

**3. Spot Instances for Non-Critical**
```
Game Servers: On-demand (critical)
Analytics: Spot instances (70% cheaper)
Batch jobs: Spot instances
```

### Incident: Traffic Spike

**Event:** Viral TikTok video â†’ 10x traffic  
**Impact:** Cluster autoscaler couldn't keep up  

**Response:**
```bash
# 1. Emergency scale
kubectl scale statefulset game-server --replicas=2000

# 2. Request more quota from cloud provider

# 3. Enable aggressive autoscaling
kubectl patch hpa game-server-hpa --patch '
spec:
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
'
```

**Prevention:**
```
âœ“ Pre-provisioned spare capacity
âœ“ Faster autoscaling policies
âœ“ Better monitoring/alerting
âœ“ Runbook for traffic spikes
```

---

## ğŸ“Š Common Patterns

### Pattern 1: Gradual Rollout

```
All migrations used:
â€¢ Feature flags
â€¢ Canary deployments
â€¢ Extensive monitoring
â€¢ Quick rollback capability
```

### Pattern 2: Observability First

```
Before production:
â€¢ Metrics collection
â€¢ Log aggregation
â€¢ Distributed tracing
â€¢ Alerting rules
â€¢ Dashboards
```

### Pattern 3: Cost Optimization

```
â€¢ Right-size resources
â€¢ Use spot instances
â€¢ Cluster autoscaling
â€¢ Namespace quotas
â€¢ Regular cost reviews
```

---

## ğŸ“ Key Lessons

**Technical:**
```
âœ“ Start with monitoring
âœ“ Automate everything
âœ“ Test failure scenarios
âœ“ Have rollback plans
âœ“ Use GitOps
```

**Process:**
```
âœ“ Gradual migrations
âœ“ Feature flags
âœ“ War room for big events
âœ“ Post-mortems
âœ“ Runbooks
```

**People:**
```
âœ“ Train team on K8s
âœ“ On-call rotation
âœ“ Clear ownership
âœ“ Documentation
âœ“ Knowledge sharing
```

---

## ğŸ’¡ Recommendations

**For Startups:**
```
â€¢ Start with managed K8s (GKE/EKS)
â€¢ Use simple architecture first
â€¢ Focus on core features
â€¢ Optimize later
```

**For Enterprises:**
```
â€¢ Gradual migration (strangler pattern)
â€¢ Multi-cluster for isolation
â€¢ Invest in platform team
â€¢ Compliance from day 1
```

**For High-Traffic:**
```
â€¢ Multi-region deployment
â€¢ Aggressive autoscaling
â€¢ Extensive load testing
â€¢ Chaos engineering
```

---

[â¬…ï¸ 11.8. Troubleshooting](./08-troubleshooting.md) | [ğŸ  Má»¥c Lá»¥c](../README.md)

