# 4.2. Deployment - Production Workload Management

> Rolling updates, rollbacks, vÃ  declarative application management

---

## ğŸ¯ Má»¥c TiÃªu Há»c

Sau khi há»c xong pháº§n nÃ y, báº¡n sáº½:
- âœ… Hiá»ƒu **Deployment lÃ  gÃ¬** vÃ  **Táº I SAO dÃ¹ng thay vÃ¬ ReplicaSet**
- âœ… Thá»±c hiá»‡n **Rolling Updates** zero-downtime
- âœ… **Rollback** khi deployment fails
- âœ… Hiá»ƒu **Deployment strategies** (RollingUpdate vs Recreate)
- âœ… **Scale applications** horizontally
- âœ… **Troubleshoot** deployments trong production

---

## ğŸ“¦ Deployment LÃ  GÃ¬?

### Äá»‹nh NghÄ©a

**Deployment** = High-level controller quáº£n lÃ½ ReplicaSets vÃ  Pods, provides declarative updates vÃ  rollback capabilities.

### Giáº£i ThÃ­ch Báº±ng VÃ­ Dá»¥

**Deployment giá»‘ng nhÆ° quáº£n lÃ½ rollout phiÃªn báº£n app:**

```
ğŸ¢ TRIá»‚N KHAI PHáº¦N Má»€M Má»šI

Old way (Manual - like bare ReplicaSet):
1. Stop old version (downtime! âŒ)
2. Deploy new version
3. Test
4. If fails: Panic! Rollback manual
â†’ Downtime, risk, stress

Deployment way (Automated):
1. Deploy new version gradually
   Old: 3 Pods â†’ 2 Pods â†’ 1 Pod â†’ 0 Pods
   New: 0 Pods â†’ 1 Pod â†’ 2 Pods â†’ 3 Pods
   â†’ Always have running Pods! (Zero downtime âœ“)

2. If new version fails:
   kubectl rollout undo deployment/app
   â†’ Automatic rollback to previous version!

3. Track history:
   â†’ Revision 1: v1.0
   â†’ Revision 2: v1.1
   â†’ Revision 3: v1.2 (current)
```

---

## ğŸ¤” Táº I SAO Cáº§n Deployment?

### ReplicaSet Limitations

```yaml
# With ReplicaSet only
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      version: v1  # Version in selector!
  template:
    metadata:
      labels:
        app: webapp
        version: v1
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
```

**Update to v2:**

```bash
# Problem: How to update to nginx:1.22?

# Option 1: Update ReplicaSet (DOESN'T WORK!)
kubectl edit rs webapp-v1
# Change image: nginx:1.22
# Save and exit
# â†’ Pods NOT updated! Still running 1.21!
# ReplicaSet doesn't recreate Pods on template change

# Option 2: Delete Pods manually
kubectl delete pods -l app=webapp
# â†’ Downtime while Pods restart!
# âŒ Manual process
# âŒ Downtime

# Option 3: Create new ReplicaSet, delete old
kubectl apply -f webapp-v2-rs.yaml  # New RS vá»›i v2
kubectl delete rs webapp-v1         # Delete old RS
# â†’ Downtime!
# âŒ No gradual rollout
# âŒ No automatic rollback
```

### Deployment Solution

```yaml
# With Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
```

**Update to v2:**

```bash
# Simple! Just update image
kubectl set image deployment/webapp nginx=nginx:1.22

# Deployment automatically:
# 1. Creates new ReplicaSet (nginx:1.22)
# 2. Gradually scales up new RS
# 3. Gradually scales down old RS
# 4. Ensures always running Pods (zero downtime!)
# 5. Saves revision history

# If problem? Rollback:
kubectl rollout undo deployment/webapp
# â†’ Automatic rollback to 1.21!

âœ“ Zero downtime
âœ“ Automated rollout
âœ“ Easy rollback
âœ“ Revision tracking
```

---

## ğŸ—ï¸ Deployment Architecture

### Three-Layer Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               DEPLOYMENT                        â”‚
â”‚  (Desired state: 3 replicas, image: nginx:1.22)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ manages
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            REPLICASETS                          â”‚
â”‚                                                 â”‚
â”‚  Old RS (nginx:1.21)         New RS (nginx:1.22)â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Replicas: 0      â”‚        â”‚ Replicas: 3     â”‚â”‚
â”‚  â”‚ (scaled down)    â”‚        â”‚ (scaled up)     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â”‚ manages
                                         â†“
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚        PODS              â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚Pod1â”‚ â”‚Pod2â”‚ â”‚Pod3â”‚  â”‚
                          â”‚  â”‚v1.22â”‚ â”‚v1.22â”‚ â”‚v1.22â”‚â”‚
                          â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment Flow

```
USER creates/updates Deployment
    â†“
Deployment Controller watches
    â†“
Creates/Updates ReplicaSet
    â†“
ReplicaSet Controller watches
    â†“
Creates/Updates Pods
    â†“
Pods running with desired state
```

---

## ğŸ“ Deployment YAML

### Complete Example

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  # Sá»‘ lÆ°á»£ng replicas
  replicas: 3
  
  # Selector Ä‘á»ƒ match Pods
  selector:
    matchLabels:
      app: webapp
  
  # Template cho Pods
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 3
  
  # Update strategy
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Max additional Pods during update
      maxUnavailable: 0  # Max unavailable Pods during update
  
  # Minimum time Pod is ready before considered available
  minReadySeconds: 10
  
  # Revision history limit
  revisionHistoryLimit: 10
```

---

## ğŸ”„ Rolling Update Strategy

### Default Strategy: RollingUpdate

**Parameters:**

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1        # Max PODs above desired (25% default)
    maxUnavailable: 0  # Max PODs unavailable during update (25% default)
```

**maxSurge:**
```
Desired replicas: 3
maxSurge: 1

During update:
â”œâ”€â”€ Max total Pods: 3 + 1 = 4
â”œâ”€â”€ Can have 4 Pods temporarily
â””â”€â”€ Extra capacity for graceful transition
```

**maxUnavailable:**
```
Desired replicas: 3
maxUnavailable: 0

During update:
â”œâ”€â”€ Min available Pods: 3 - 0 = 3
â”œâ”€â”€ Must always have 3 Pods running
â””â”€â”€ Zero downtime guaranteed!
```

### Rolling Update Process

**Step-by-step workflow:**

```
Initial state:
Desired: 3 replicas, image: nginx:1.21
Old RS: 3 Pods (nginx:1.21)
New RS: 0 Pods

User updates: image â†’ nginx:1.22
    â†“
Step 1: Deployment creates new ReplicaSet
Old RS: 3 Pods (nginx:1.21)
New RS: 0 Pods (nginx:1.22) â† Created!
    â†“
Step 2: Scale up new RS by 1 (maxSurge: 1)
Old RS: 3 Pods (nginx:1.21)
New RS: 1 Pod (nginx:1.22) â† Creating
Total: 4 Pods (temporary)
    â†“
Step 3: Wait for new Pod Ready
New RS: 1 Pod (nginx:1.22) â† Running + Ready!
    â†“
Step 4: Scale down old RS by 1 (maxUnavailable: 0)
Old RS: 2 Pods (nginx:1.21) â† 1 Terminated
New RS: 1 Pod (nginx:1.22)
Total: 3 Pods (back to desired)
    â†“
Step 5: Scale up new RS by 1
Old RS: 2 Pods (nginx:1.21)
New RS: 2 Pods (nginx:1.22) â† Creating
Total: 4 Pods (temporary)
    â†“
Step 6: Wait for Ready, then scale down old
Old RS: 1 Pod (nginx:1.21) â† 1 Terminated
New RS: 2 Pods (nginx:1.22)
Total: 3 Pods
    â†“
Step 7: Repeat until complete
Old RS: 0 Pods (nginx:1.21) â† Fully scaled down
New RS: 3 Pods (nginx:1.22) â† Fully scaled up
Total: 3 Pods

âœ“ Update complete!
âœ“ Zero downtime (always 3 Pods available)
```

### Visual Timeline

```
Time    Old RS (1.21)    New RS (1.22)    Total
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t0      â–ˆâ–ˆâ–ˆ             -                3
t1      â–ˆâ–ˆâ–ˆ             â–ˆ (creating)     4
t2      â–ˆâ–ˆâ–ˆ             â–ˆ (ready)        4
t3      â–ˆâ–ˆ              â–ˆ (terminate 1)  3
t4      â–ˆâ–ˆ              â–ˆâ–ˆ (creating)    4
t5      â–ˆâ–ˆ              â–ˆâ–ˆ (ready)       4
t6      â–ˆ               â–ˆâ–ˆ (terminate 1) 3
t7      â–ˆ               â–ˆâ–ˆâ–ˆ (creating)   4
t8      â–ˆ               â–ˆâ–ˆâ–ˆ (ready)      4
t9      -               â–ˆâ–ˆâ–ˆ (terminate 1)3

Final:  Old=0           New=3            3 âœ“
```

---

## ğŸ® Hands-On: Deployment Lifecycle

### Create Deployment

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
```

```bash
# Create Deployment
kubectl apply -f deployment.yaml

# Output:
# deployment.apps/nginx-deployment created

# Check Deployment
kubectl get deployments
# or shorter
kubectl get deploy

# Output:
# NAME               READY   UP-TO-DATE   AVAILABLE   AGE
# nginx-deployment   3/3     3            3           30s

# Check ReplicaSets (created by Deployment)
kubectl get rs

# Output:
# NAME                          DESIRED   CURRENT   READY   AGE
# nginx-deployment-7d4b7c9d8f   3         3         3       30s

# Check Pods (created by ReplicaSet)
kubectl get pods

# Output:
# NAME                                READY   STATUS    RESTARTS   AGE
# nginx-deployment-7d4b7c9d8f-abc12   1/1     Running   0          30s
# nginx-deployment-7d4b7c9d8f-def34   1/1     Running   0          30s
# nginx-deployment-7d4b7c9d8f-ghi56   1/1     Running   0          30s
```

---

### Update Deployment (Rolling Update)

**Method 1: kubectl set image**

```bash
# Update image tá»« 1.21 â†’ 1.22
kubectl set image deployment/nginx-deployment nginx=nginx:1.22

# Output:
# deployment.apps/nginx-deployment image updated

# Watch rollout status
kubectl rollout status deployment/nginx-deployment

# Output:
# Waiting for deployment "nginx-deployment" rollout to finish: 1 out of 3 new replicas have been updated...
# Waiting for deployment "nginx-deployment" rollout to finish: 1 old replicas are pending termination...
# Waiting for deployment "nginx-deployment" rollout to finish: 2 out of 3 new replicas have been updated...
# Waiting for deployment "nginx-deployment" rollout to finish: 2 old replicas are pending termination...
# Waiting for deployment "nginx-deployment" rollout to finish: 1 old replicas are pending termination...
# deployment "nginx-deployment" successfully rolled out

# Verify
kubectl get rs

# Output: 2 ReplicaSets!
# NAME                          DESIRED   CURRENT   READY   AGE
# nginx-deployment-7d4b7c9d8f   0         0         0       5m    â† Old (1.21)
# nginx-deployment-5d9f7b8c6e   3         3         3       1m    â† New (1.22)

kubectl describe deployment nginx-deployment | grep Image
# Image: nginx:1.22
```

**Method 2: kubectl edit**

```bash
# Edit Deployment YAML interactively
kubectl edit deployment nginx-deployment

# In editor, change:
# spec.template.spec.containers[0].image: nginx:1.22 â†’ nginx:1.23

# Save and exit
# Deployment automatically triggers rollout!

# Watch
kubectl rollout status deployment/nginx-deployment
```

**Method 3: Update YAML file vÃ  apply**

```yaml
# deployment.yaml (updated)
spec:
  template:
    spec:
      containers:
      - name: nginx
        image: nginx:1.23  # Changed from 1.22
```

```bash
# Apply changes
kubectl apply -f deployment.yaml

# Deployment detects difference vÃ  rolls out!
kubectl rollout status deployment/nginx-deployment
```

---

### Watch Rolling Update in Real-Time

```bash
# Terminal 1: Watch Pods
kubectl get pods -w

# Terminal 2: Trigger update
kubectl set image deployment/nginx-deployment nginx=nginx:1.23

# Terminal 1 shows:
# NAME                                READY   STATUS              AGE
# nginx-deployment-5d9f7b8c6e-abc12   1/1     Running             5m
# nginx-deployment-5d9f7b8c6e-def34   1/1     Running             5m
# nginx-deployment-5d9f7b8c6e-ghi56   1/1     Running             5m
# nginx-deployment-6e8g8d9f7e-jkl78   0/1     ContainerCreating   2s  â† New!
# nginx-deployment-6e8g8d9f7e-jkl78   1/1     Running             5s  â† Ready!
# nginx-deployment-5d9f7b8c6e-abc12   1/1     Terminating         5m  â† Old terminating
# nginx-deployment-6e8g8d9f7e-mno90   0/1     ContainerCreating   3s  â† New!
# ... continues until all updated ...
```

---

### Pause vÃ  Resume Rollout

```bash
# Pause rollout (useful for canary testing)
kubectl rollout pause deployment/nginx-deployment

# Update image
kubectl set image deployment/nginx-deployment nginx=nginx:1.24

# Deployment created new RS but paused!
kubectl get rs
# New RS exists nhÆ°ng replicas = 0

# Check a subset of Pods vá»›i new version
# ... test/monitor ...

# Resume rollout
kubectl rollout resume deployment/nginx-deployment

# Continues rolling update!
kubectl rollout status deployment/nginx-deployment
```

---

### Rollback Deployment

**View Rollout History**

```bash
# List revision history
kubectl rollout history deployment/nginx-deployment

# Output:
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         <none>
# 3         <none>

# View specific revision details
kubectl rollout history deployment/nginx-deployment --revision=2

# Output:
# deployment.apps/nginx-deployment with revision #2
# Pod Template:
#   Labels:       app=nginx
#   Containers:
#    nginx:
#     Image:      nginx:1.22
#     Port:       80/TCP
#     ...
```

**Record Change Cause**

```bash
# Update vá»›i --record flag (adds to CHANGE-CAUSE)
kubectl set image deployment/nginx-deployment nginx=nginx:1.23 --record

# History now shows:
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         <none>
# 3         kubectl set image deployment/nginx-deployment nginx=nginx:1.23 --record
```

**Rollback to Previous Version**

```bash
# Rollback to previous revision
kubectl rollout undo deployment/nginx-deployment

# Output:
# deployment.apps/nginx-deployment rolled back

# Check status
kubectl rollout status deployment/nginx-deployment

# Verify image
kubectl describe deployment nginx-deployment | grep Image
# Image: nginx:1.22 (back to previous!)

# History updates:
kubectl rollout history deployment/nginx-deployment
# REVISION  CHANGE-CAUSE
# 1         <none>
# 3         kubectl set image...
# 4         <none>  â† Rollback created new revision!
```

**Rollback to Specific Revision**

```bash
# Rollback to revision 1
kubectl rollout undo deployment/nginx-deployment --to-revision=1

# Back to original image!
kubectl describe deployment nginx-deployment | grep Image
# Image: nginx:1.21
```

---

## âš™ï¸ Deployment Strategies

### Strategy 1: RollingUpdate (Default)

```yaml
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
```

**Use cases:**
- âœ“ Stateless applications
- âœ“ Zero downtime requirement
- âœ“ Gradual rollout preferred
- âœ“ Quick rollback capability

**Example configurations:**

```yaml
# Conservative (safest, slowest)
maxSurge: 1           # Add 1 at a time
maxUnavailable: 0     # Never below desired

# Balanced
maxSurge: 25%         # Add 25% extra
maxUnavailable: 25%   # Can lose 25%

# Aggressive (fastest, riskier)
maxSurge: 100%        # Double Pods temporarily
maxUnavailable: 50%   # Half can be down
```

---

### Strategy 2: Recreate

```yaml
spec:
  strategy:
    type: Recreate
```

**Process:**
```
1. Delete ALL old Pods
2. Wait for all terminated
3. Create all new Pods
4. Wait for all running

â†’ Downtime between step 2 and 4!
```

**Use cases:**
- âœ“ Stateful applications that can't run multiple versions
- âœ“ Database migrations required
- âœ“ Shared resources (can't have old vÃ  new together)
- âœ“ Downtime acceptable

**Example:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database-app
spec:
  replicas: 1
  strategy:
    type: Recreate  # Terminate old before new
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      containers:
      - name: postgres
        image: postgres:14
```

```bash
# Update triggers Recreate strategy
kubectl set image deployment/database-app postgres=postgres:15

# Behavior:
# 1. Old Pod: Running â†’ Terminating â†’ Terminated
# 2. Downtime period (no Pods running!)
# 3. New Pod: Creating â†’ Running
```

---

## ğŸ“Š Scaling Deployments

### Manual Scaling

```bash
# Scale up to 5 replicas
kubectl scale deployment nginx-deployment --replicas=5

# Verify
kubectl get deployment nginx-deployment
# READY: 5/5

kubectl get pods -l app=nginx
# 5 Pods running

# Scale down to 2
kubectl scale deployment nginx-deployment --replicas=2

# 3 Pods terminated
kubectl get pods -l app=nginx
# 2 Pods running
```

### Autoscaling (HPA)

```bash
# Create Horizontal Pod Autoscaler
kubectl autoscale deployment nginx-deployment --min=3 --max=10 --cpu-percent=80

# HPA scales based on CPU usage
# CPU > 80% â†’ Scale up (max 10)
# CPU < 80% â†’ Scale down (min 3)

# Check HPA
kubectl get hpa

# Output:
# NAME               REFERENCE                     TARGETS   MINPODS   MAXPODS   REPLICAS
# nginx-deployment   Deployment/nginx-deployment   50%/80%   3         10        3
```

---

## ğŸ› Troubleshooting Deployments

### Issue 1: Deployment Stuck (Progressing)

```bash
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   2/3     2            2           10m

# Stuck at 2/3 for long time

# Check Deployment events
$ kubectl describe deployment nginx-deployment

# Events might show:
# ReplicaSet "nginx-deployment-xyz" has timed out progressing

# Check Pod status
$ kubectl get pods -l app=nginx

# NAME                                READY   STATUS             RESTARTS   AGE
# nginx-deployment-xyz-abc12          1/1     Running            0          10m
# nginx-deployment-xyz-def34          1/1     Running            0          10m
# nginx-deployment-xyz-ghi56          0/1     ImagePullBackOff   0          10m

# Fix: Check Pod errors (image issues, resource limits, etc.)
```

---

### Issue 2: Rollout Failed (Bad Image)

```bash
# Update to non-existent image
kubectl set image deployment/nginx-deployment nginx=nginx:wrongtag

# Rollout starts but Pods fail
kubectl get pods

# NAME                                READY   STATUS             RESTARTS   AGE
# nginx-deployment-bad123-abc        0/1     ImagePullBackOff    0          2m
# nginx-deployment-old456-def        1/1     Running             0          10m
# nginx-deployment-old456-ghi        1/1     Running             0          10m
# nginx-deployment-old456-jkl        1/1     Running             0          10m

# Old Pods still running! (maxUnavailable: 0)
# â†’ Application still available âœ“

# Fix: Rollback
kubectl rollout undo deployment/nginx-deployment

# Or fix image
kubectl set image deployment/nginx-deployment nginx=nginx:1.22
```

---

### Issue 3: Deployment Doesn't Update Pods

```bash
# Update Deployment
kubectl edit deployment nginx-deployment
# Changed some config

# But Pods don't restart!

# Reason: Only template changes trigger rollout
# Changes to replicas, labels, etc. don't restart Pods

# Template changes that trigger rollout:
âœ“ Image change
âœ“ Env vars change
âœ“ Resource limits change
âœ“ Volume mounts change
âœ“ Command/args change

# Non-template changes (no rollout):
âœ— Replicas change (scales, doesn't restart)
âœ— Deployment labels (metadata, not template)
âœ— Strategy change (doesn't affect running Pods)

# Force rollout:
kubectl rollout restart deployment/nginx-deployment
# Triggers rolling restart of all Pods
```

---

### Issue 4: Too Many ReplicaSets

```bash
$ kubectl get rs

# Output: 15 old ReplicaSets!
# NAME                          DESIRED   CURRENT   READY   AGE
# nginx-deployment-abc123       0         0         0       30d
# nginx-deployment-def456       0         0         0       29d
# nginx-deployment-ghi789       0         0         0       28d
# ... 12 more old ReplicaSets ...
# nginx-deployment-xyz999       3         3         3       1d  â† Current

# Reason: revisionHistoryLimit not set (default 10)

# Fix: Set limit
kubectl edit deployment nginx-deployment

# Add/update:
spec:
  revisionHistoryLimit: 3  # Keep only 3 old ReplicaSets

# Or in YAML:
apiVersion: apps/v1
kind: Deployment
spec:
  revisionHistoryLimit: 3
```

---

## ğŸ“ Kiá»ƒm Tra Hiá»ƒu Biáº¿t

**1. Deployment vs ReplicaSet - khÃ¡c nhau gÃ¬?**
<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

**ReplicaSet:**
- Low-level controller
- Only maintains replica count
- No update management
- No rollback
- Directly manages Pods

**Deployment:**
- High-level controller
- Manages ReplicaSets
- Rolling updates
- Automatic rollback
- Revision history
- Declarative updates

**Use:** Always use Deployment in production!
</details>

**2. maxSurge vÃ  maxUnavailable lÃ m gÃ¬?**
<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

**maxSurge:**
- Max Pods above desired during update
- Example: replicas=3, maxSurge=1 â†’ Max 4 Pods during update
- Higher = Faster rollout (more resources)

**maxUnavailable:**
- Max Pods below desired during update
- Example: replicas=3, maxUnavailable=0 â†’ Always 3 Pods available
- 0 = Zero downtime guarantee

**Balance:**
- maxSurge=1, maxUnavailable=0: Safe, zero downtime
- maxSurge=50%, maxUnavailable=50%: Fast, more risk
</details>

**3. Khi nÃ o dÃ¹ng Recreate strategy?**
<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

Use Recreate when:
- Can't run old vÃ  new versions simultaneously
- Database schema changes required
- Shared resources conflict
- Downtime is acceptable

Examples:
- Stateful databases
- Applications vá»›i breaking changes
- Resource-constrained environments

Don't use for:
- Stateless web apps (use RollingUpdate)
- Production vá»›i zero downtime requirement
</details>

---

## ğŸ’ª BÃ i Táº­p Thá»±c HÃ nh

### BÃ i 1: Complete Deployment Lifecycle

```yaml
# app-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
```

```bash
# 1. Deploy v1.21
kubectl apply -f app-deployment.yaml

# 2. Verify
kubectl get deploy,rs,pods

# 3. Update to v1.22
kubectl set image deployment/webapp nginx=nginx:1.22 --record

# 4. Watch rollout
kubectl rollout status deployment/webapp

# 5. Verify new version
kubectl describe deployment webapp | grep Image

# 6. Update to v1.23
kubectl set image deployment/webapp nginx=nginx:1.23 --record

# 7. Check history
kubectl rollout history deployment/webapp

# 8. Rollback to v1.22
kubectl rollout undo deployment/webapp

# 9. Verify rolled back
kubectl describe deployment webapp | grep Image

# 10. Cleanup
kubectl delete deployment webapp
```

---

### BÃ i 2: Test Different Strategies

```bash
# Create Deployment vá»›i RollingUpdate
kubectl create deployment test-rolling --image=nginx:1.21 --replicas=5

# Watch Pods trong terminal 1
kubectl get pods -w -l app=test-rolling

# Update in terminal 2
kubectl set image deployment/test-rolling nginx=nginx:1.22

# Observe: Gradual rollout

# Now test Recreate
kubectl create deployment test-recreate --image=nginx:1.21 --replicas=5

# Change strategy
kubectl patch deployment test-recreate -p '{"spec":{"strategy":{"type":"Recreate"}}}'

# Watch Pods
kubectl get pods -w -l app=test-recreate

# Update
kubectl set image deployment/test-recreate nginx=nginx:1.22

# Observe: All Pods terminate, then new Pods create (downtime!)

# Cleanup
kubectl delete deployment test-rolling test-recreate
```

---

## ğŸ¯ Key Takeaways

1. **Deployment = Production Standard**
   - Always use for stateless apps
   - ReplicaSet + Updates + Rollbacks
   - Declarative management

2. **Rolling Update = Zero Downtime**
   - Gradual rollout (scale up new, scale down old)
   - maxSurge vÃ  maxUnavailable control speed/risk
   - Always have available Pods

3. **Rollback = Safety Net**
   - Easy rollback: kubectl rollout undo
   - Revision history tracked
   - Can rollback to specific revision

4. **Two Strategies**
   - RollingUpdate: Gradual, zero downtime (default)
   - Recreate: Terminate all, then create (downtime)

5. **Best Practices**
   - Use --record for change tracking
   - Set appropriate maxSurge/maxUnavailable
   - Configure revisionHistoryLimit
   - Add readiness probes
   - Test rollbacks in staging

---

## ğŸš€ Tiáº¿p Theo

Deployment mastered! Next: StatefulSets cho stateful applications!

**Next:** [4.3. StatefulSet â†’](./03-statefulset.md)

---

[â¬…ï¸ 4.1. ReplicaSet](./01-replicaset.md) | [ğŸ  Má»¥c Lá»¥c](../README.md) | [ğŸ“‚ Pháº§n 4: Workloads](./README.md) | [â¡ï¸ 4.3. StatefulSet](./03-statefulset.md)
