# 4.4. DaemonSet - Pod Tr√™n M·ªói Node

> Run exactly one Pod on every Node in cluster

---

## üéØ M·ª•c Ti√™u H·ªçc

Sau khi h·ªçc xong ph·∫ßn n√†y, b·∫°n s·∫Ω:
- ‚úÖ Hi·ªÉu **DaemonSet l√† g√¨** v√† **T·∫†I SAO c·∫ßn**
- ‚úÖ Bi·∫øt **use cases** ph·ªï bi·∫øn cho DaemonSet
- ‚úÖ Deploy **system daemons** (logging, monitoring)
- ‚úÖ Control **which Nodes** run DaemonSet Pods
- ‚úÖ **Update v√† rollback** DaemonSets
- ‚úÖ **Troubleshoot** DaemonSet issues

---

## üì¶ DaemonSet L√† G√¨?

### ƒê·ªãnh Nghƒ©a

**DaemonSet** = Controller ensures m·ªôt Pod copy ch·∫°y tr√™n m·ªói Node (ho·∫∑c subset of Nodes) trong cluster.

### Gi·∫£i Th√≠ch B·∫±ng V√≠ D·ª•

**DaemonSet gi·ªëng nh∆∞ security guards:**

```
üè¢ T√íA NH√Ä VƒÇN PH√íNG (Cluster)

DEPLOYMENT = Team l√†m d·ª± √°n:
‚îú‚îÄ‚îÄ 3 developers work together
‚îú‚îÄ‚îÄ C√≥ th·ªÉ ·ªü b·∫•t k·ª≥ t·∫ßng n√†o
‚îú‚îÄ‚îÄ Kh√¥ng c·∫ßn m·ªói t·∫ßng c√≥ 1 ng∆∞·ªùi
‚îî‚îÄ‚îÄ Scale up/down based on workload

Use cases: Application servers

DAEMONSET = Security guards:
‚îú‚îÄ‚îÄ M·ªñI T·∫¶NG (Node) ph·∫£i c√≥ 1 guard
‚îú‚îÄ‚îÄ T·∫ßng m·ªõi (new Node) ‚Üí Deploy 1 guard automatically
‚îú‚îÄ‚îÄ T·∫ßng ƒë√≥ng c·ª≠a (Node removed) ‚Üí Guard removed
‚îî‚îÄ‚îÄ Always 1 guard per floor, no more, no less

Use cases: Monitoring, logging, network agents
```

---

## ü§î T·∫†I SAO C·∫ßn DaemonSet?

### Problem Without DaemonSet

**Scenario: Logging agents**

```bash
# Using Deployment
kubectl create deployment log-collector --image=fluentd --replicas=3

# Problems:
‚ùå 3 Pods random placement (maybe all on 1 Node!)
   Node 1: 2 log-collector Pods
   Node 2: 1 log-collector Pod
   Node 3: 0 log-collector Pods ‚Üê No logging!

‚ùå Need logging on ALL Nodes
   ‚Üí Must manually calculate replicas = number of Nodes
   ‚Üí Nodes added/removed ‚Üí Manual updates

‚ùå Pods can be evicted/rescheduled
   ‚Üí Node might temporarily have 0 Pods
   ‚Üí Missing logs during that time

‚ùå Can't guarantee 1 Pod per Node
```

### Solution: DaemonSet

```yaml
# Using DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-collector
spec:
  selector:
    matchLabels:
      app: log-collector
  template:
    spec:
      containers:
      - name: fluentd
        image: fluentd

# Benefits:
‚úì Exactly 1 Pod per Node
  Node 1: 1 log-collector Pod
  Node 2: 1 log-collector Pod
  Node 3: 1 log-collector Pod

‚úì Automatic scaling
  Node 4 added ‚Üí Pod automatically created
  Node 2 removed ‚Üí Pod automatically deleted

‚úì Guaranteed coverage
  Every Node has logging agent
  No gaps in log collection

‚úì No manual replica management
  DaemonSet handles it automatically
```

---

## üéØ Common Use Cases

### 1. Log Collection

**Collect logs t·ª´ m·ªçi Node:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

**Why DaemonSet?**
- Need to collect logs t·ª´ ALL Nodes
- Each Node's logs stored locally on Node
- Agent must run on each Node to access logs

---

### 2. Monitoring Agents

**Collect metrics t·ª´ m·ªçi Node:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      containers:
      - name: node-exporter
        image: prom/node-exporter
        ports:
        - containerPort: 9100
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
      hostNetwork: true  # Access Node's network
      hostPID: true      # Access Node's processes
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
```

**Why DaemonSet?**
- Monitor CPU, memory, disk c·ªßa m·ªói Node
- Metrics specific to each Node
- Need agent on every Node

---

### 3. Network Plugins (CNI)

**Network overlay agents:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-proxy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: kube-proxy
  template:
    spec:
      containers:
      - name: kube-proxy
        image: k8s.gcr.io/kube-proxy
      hostNetwork: true  # Access Node network
```

**Examples:**
- kube-proxy (K8s networking)
- Calico, Flannel, Weave (CNI plugins)
- Load balancer agents

**Why DaemonSet?**
- Network configuration needed on every Node
- Pods on each Node need network access
- Can't function without agent on Node

---

### 4. Storage Plugins

**Storage drivers:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: csi-driver
spec:
  template:
    spec:
      containers:
      - name: csi-node
        image: my-csi-driver
        volumeMounts:
        - name: plugin-dir
          mountPath: /var/lib/kubelet/plugins
      volumes:
      - name: plugin-dir
        hostPath:
          path: /var/lib/kubelet/plugins
```

**Examples:**
- CSI node plugins
- GlusterFS, Ceph clients
- Cloud provider storage drivers

---

## üìù DaemonSet YAML

### Basic Example

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-agent
  labels:
    app: monitoring
spec:
  # Selector to match Pods
  selector:
    matchLabels:
      app: monitoring-agent
  
  # Pod template
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
      - name: agent
        image: monitoring-agent:v1
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        ports:
        - containerPort: 9090
          name: metrics
```

### With hostPath Volumes

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-collector
spec:
  selector:
    matchLabels:
      app: log-collector
  template:
    metadata:
      labels:
        app: log-collector
    spec:
      containers:
      - name: fluentd
        image: fluentd:v1
        volumeMounts:
        - name: varlog
          mountPath: /var/log  # Mount Node's /var/log
        - name: containers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: varlog
        hostPath:
          path: /var/log  # Node's filesystem
      - name: containers
        hostPath:
          path: /var/lib/docker/containers
```

---

## üéÆ Hands-On: Working v·ªõi DaemonSets

### Create DaemonSet

```yaml
# daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemon
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx-daemon
  template:
    metadata:
      labels:
        app: nginx-daemon
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

```bash
# Create DaemonSet
kubectl apply -f daemonset.yaml

# Output:
# daemonset.apps/nginx-daemon created

# Check DaemonSet
kubectl get daemonset
# or shorter
kubectl get ds

# Output:
# NAME           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
# nginx-daemon   3         3         3       3            3           <none>          30s
#                ‚Üë         ‚Üë
#                Number of Nodes

# Check Pods (one per Node!)
kubectl get pods -o wide

# Output:
# NAME                 READY   STATUS    NODE      IP
# nginx-daemon-abc12   1/1     Running   node-1    10.244.1.5
# nginx-daemon-def34   1/1     Running   node-2    10.244.2.8
# nginx-daemon-ghi56   1/1     Running   node-3    10.244.3.9

# Exactly 1 Pod per Node!
```

---

### Test Node Addition

```bash
# Add new Node to cluster (minikube example)
minikube node add

# DaemonSet automatically creates Pod on new Node!
kubectl get pods -o wide

# Output:
# NAME                 READY   STATUS    NODE      IP
# nginx-daemon-abc12   1/1     Running   node-1    10.244.1.5
# nginx-daemon-def34   1/1     Running   node-2    10.244.2.8
# nginx-daemon-ghi56   1/1     Running   node-3    10.244.3.9
# nginx-daemon-jkl78   1/1     Running   node-4    10.244.4.10  ‚Üê NEW!

# Pod automatically created on node-4!
```

---

### Node Selector

**Run DaemonSet only on specific Nodes:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      # Only run on Nodes v·ªõi label disktype=ssd
      nodeSelector:
        disktype: ssd
      containers:
      - name: monitor
        image: ssd-monitor:v1
```

```bash
# Label some Nodes
kubectl label nodes node-1 disktype=ssd
kubectl label nodes node-2 disktype=ssd

# Create DaemonSet
kubectl apply -f ssd-monitor-daemonset.yaml

# Pods only on node-1 v√† node-2!
kubectl get pods -o wide
# NAME                 READY   STATUS    NODE
# ssd-monitor-abc12    1/1     Running   node-1
# ssd-monitor-def34    1/1     Running   node-2
# (No Pod on node-3 - doesn't have disktype=ssd label)

# Add label to node-3
kubectl label nodes node-3 disktype=ssd

# Pod automatically created on node-3!
kubectl get pods -o wide
# ssd-monitor-ghi56    1/1     Running   node-3  ‚Üê NEW!
```

---

### Tolerations cho Master Nodes

**By default: DaemonSet Pods DON'T run on Master Nodes**

```bash
# Check Master Node taints
kubectl describe node master-node | grep Taints

# Output:
# Taints: node-role.kubernetes.io/master:NoSchedule

# Master has taint ‚Üí Regular Pods can't schedule
```

**Run DaemonSet on Master:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-everywhere
spec:
  selector:
    matchLabels:
      app: monitoring
  template:
    metadata:
      labels:
        app: monitoring
    spec:
      # Tolerate Master Node taint
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      # Also tolerate control-plane taint (newer K8s)
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      containers:
      - name: agent
        image: monitoring-agent:v1
```

```bash
# Apply
kubectl apply -f monitoring-daemonset.yaml

# Pods on ALL Nodes including Master!
kubectl get pods -o wide
# NAME                       READY   STATUS    NODE
# monitoring-abc12           1/1     Running   master-node  ‚Üê On Master!
# monitoring-def34           1/1     Running   worker-1
# monitoring-ghi56           1/1     Running   worker-2
```

---

### Update DaemonSet

**Update strategy:**

```yaml
spec:
  updateStrategy:
    type: RollingUpdate  # Default
    rollingUpdate:
      maxUnavailable: 1  # Max Pods down during update
```

**Update image:**

```bash
# Update image
kubectl set image daemonset/nginx-daemon nginx=nginx:1.22

# Pods updated one Node at a time
kubectl get pods -w

# Output:
# nginx-daemon-abc12   1/1     Terminating   node-1
# nginx-daemon-abc12   0/1     ContainerCreating   node-1
# nginx-daemon-abc12   1/1     Running   node-1
# nginx-daemon-def34   1/1     Terminating   node-2  ‚Üê Next Node
# ...

# Check status
kubectl rollout status daemonset/nginx-daemon

# Output:
# daemon set "nginx-daemon" successfully rolled out
```

**OnDelete strategy:**

```yaml
spec:
  updateStrategy:
    type: OnDelete  # Manual control
```

```bash
# Update DaemonSet
kubectl set image daemonset/nginx-daemon nginx=nginx:1.22

# Pods NOT updated automatically!

# Manually delete Pods to trigger update
kubectl delete pod nginx-daemon-abc12
# Pod recreated with new image

kubectl delete pod nginx-daemon-def34
# Pod recreated with new image
```

---

### Delete DaemonSet

```bash
# Delete DaemonSet
kubectl delete daemonset nginx-daemon

# All Pods deleted
kubectl get pods -l app=nginx-daemon
# No resources found

# DaemonSet deleted
kubectl get daemonset nginx-daemon
# Error from server (NotFound)
```

---

## üîß Advanced Configurations

### hostNetwork: true

**Use Node's network namespace:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: network-monitor
spec:
  template:
    spec:
      hostNetwork: true  # Use Node's network
      containers:
      - name: monitor
        image: network-monitor:v1
        ports:
        - containerPort: 9100  # Binds to Node's port 9100!
```

**Use cases:**
- Network monitoring tools
- kube-proxy
- CNI plugins

**‚ö†Ô∏è Warning:** Pod exposes ports directly on Node!

---

### hostPID v√† hostIPC

**Access Node's processes:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: system-monitor
spec:
  template:
    spec:
      hostPID: true   # See Node's processes
      hostIPC: true   # Access Node's IPC
      containers:
      - name: monitor
        image: system-monitor:v1
```

**Use cases:**
- System monitoring (CPU, memory per process)
- Debugging tools
- Security scanning

---

### Privileged Containers

**Run v·ªõi elevated privileges:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: privileged-agent
spec:
  template:
    spec:
      containers:
      - name: agent
        image: privileged-agent:v1
        securityContext:
          privileged: true  # Full access to Node
        volumeMounts:
        - name: dev
          mountPath: /dev
      volumes:
      - name: dev
        hostPath:
          path: /dev
```

**Use cases:**
- Storage drivers
- Network plugins
- System-level tools

**‚ö†Ô∏è Security risk:** Use only when necessary!

---

## üêõ Troubleshooting DaemonSets

### Issue 1: Pods Not on All Nodes

```bash
$ kubectl get daemonset
NAME           DESIRED   CURRENT   READY
nginx-daemon   5         3         3

# Only 3/5 Nodes have Pods

# Check Pod status
$ kubectl get pods -l app=nginx-daemon -o wide

# Describe DaemonSet
$ kubectl describe daemonset nginx-daemon

# Events might show:
# Warning  FailedCreate  Pod nginx-daemon-xyz is forbidden: 
# unable to validate against any pod security policy

# Possible causes:
1. Node taints (Pods can't tolerate)
2. NodeSelector mismatch
3. Resource constraints
4. Pod Security Policy blocking

# Debug steps:
kubectl describe nodes  # Check taints
kubectl get nodes --show-labels  # Check labels
```

---

### Issue 2: Pods CrashLoopBackOff

```bash
$ kubectl get pods -l app=log-collector
NAME                  READY   STATUS             RESTARTS   AGE
log-collector-abc12   0/1     CrashLoopBackOff   5          5m

# DaemonSet Pod crashing on some Nodes

# Check logs
$ kubectl logs log-collector-abc12

# Possible causes:
1. hostPath volume doesn't exist on Node
2. Permission issues (need privileged: true)
3. Resource limits too low
4. Application configuration error

# Fix examples:
# - Add privileged: true if needed
# - Adjust resource limits
# - Fix hostPath paths
```

---

### Issue 3: Can't Delete DaemonSet

```bash
$ kubectl delete daemonset nginx-daemon
# Hangs...

# Check finalizers
$ kubectl get daemonset nginx-daemon -o yaml | grep finalizers

# Force delete if stuck
$ kubectl delete daemonset nginx-daemon --force --grace-period=0

# Or patch to remove finalizers
$ kubectl patch daemonset nginx-daemon -p '{"metadata":{"finalizers":[]}}' --type=merge
```

---

## üéì Ki·ªÉm Tra Hi·ªÉu Bi·∫øt

**1. DaemonSet vs Deployment - khi n√†o d√πng g√¨?**
<details>
<summary>Xem ƒë√°p √°n</summary>

**DaemonSet:**
- Use when: Need exactly 1 Pod per Node
- Examples: Logging, monitoring, network agents
- Scaling: Automatic (# Pods = # Nodes)

**Deployment:**
- Use when: Need N replicas (can be anywhere)
- Examples: Web apps, APIs, workers
- Scaling: Manual or HPA

**Rule:** If need "every Node" ‚Üí DaemonSet. Otherwise ‚Üí Deployment.
</details>

**2. L√†m sao run DaemonSet on Master Nodes?**
<details>
<summary>Xem ƒë√°p √°n</summary>

Add tolerations:

```yaml
spec:
  template:
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
```

Master Nodes have NoSchedule taints by default. Tolerations allow Pods to schedule despite taints.
</details>

**3. DaemonSet c√≥ auto-scale khi add Nodes kh√¥ng?**
<details>
<summary>Xem ƒë√°p √°n</summary>

**YES! Automatic.**

```
Cluster: 3 Nodes ‚Üí DaemonSet: 3 Pods

Add Node 4 ‚Üí DaemonSet automatically: 4 Pods
Remove Node 2 ‚Üí DaemonSet automatically: 3 Pods

No manual intervention needed!
```

This is THE reason DaemonSets exist - automatic per-Node deployment.
</details>

---

## üí™ B√†i T·∫≠p Th·ª±c H√†nh

### B√†i 1: Deploy Monitoring DaemonSet

```yaml
# monitoring-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-monitor
spec:
  selector:
    matchLabels:
      app: node-monitor
  template:
    metadata:
      labels:
        app: node-monitor
    spec:
      containers:
      - name: monitor
        image: busybox
        command: ['sh', '-c', 'while true; do hostname; sleep 30; done']
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
```

```bash
# 1. Deploy
kubectl apply -f monitoring-daemonset.yaml

# 2. Verify (1 Pod per Node)
kubectl get pods -o wide

# 3. Check logs from different Pods
kubectl logs -l app=node-monitor --tail=5

# 4. Update image
kubectl set image daemonset/node-monitor monitor=busybox:1.35

# 5. Watch rollout
kubectl rollout status daemonset/node-monitor

# 6. Cleanup
kubectl delete daemonset node-monitor
```

---

## üéØ Key Takeaways

1. **DaemonSet = One Pod Per Node**
   - Exactly 1 Pod on each Node
   - Automatic scaling v·ªõi cluster

2. **Use Cases**
   - Logging agents (Fluentd)
   - Monitoring (Node Exporter)
   - Network (CNI plugins, kube-proxy)
   - Storage (CSI drivers)

3. **Node Selection**
   - nodeSelector: Run on specific Nodes
   - tolerations: Run on tainted Nodes (Master)
   - Default: All worker Nodes

4. **Updates**
   - RollingUpdate: Automatic, one Node at a time
   - OnDelete: Manual control

5. **vs Deployment**
   - DaemonSet: Per-Node daemons
   - Deployment: N replicas anywhere

---

## üöÄ Ti·∫øp Theo

DaemonSet mastered! Next: Jobs & CronJobs - batch workloads!

**Next:** [4.5. Jobs & CronJobs ‚Üí](./05-jobs-cronjobs.md)

---

[‚¨ÖÔ∏è 4.3. StatefulSet](./03-statefulset.md) | [üè† M·ª•c L·ª•c](../README.md) | [üìÇ Ph·∫ßn 4: Workloads](./README.md) | [‚û°Ô∏è 4.5. Jobs](./05-jobs-cronjobs.md)
