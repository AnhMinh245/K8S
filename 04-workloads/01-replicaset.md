# 4.1. ReplicaSet - Duy TrÃ¬ Sá»‘ LÆ°á»£ng Pods

> Äáº£m báº£o desired sá»‘ Pods luÃ´n running

---

## ğŸ¯ Má»¥c TiÃªu Há»c

Sau khi há»c xong pháº§n nÃ y, báº¡n sáº½:
- âœ… Hiá»ƒu **ReplicaSet lÃ  gÃ¬** vÃ  **Táº I SAO cáº§n**
- âœ… Biáº¿t **cÃ¡ch ReplicaSet hoáº¡t Ä‘á»™ng** (reconciliation loop)
- âœ… Táº¡o vÃ  quáº£n lÃ½ **ReplicaSets**
- âœ… Hiá»ƒu **relationship vá»›i Pods vÃ  Deployments**
- âœ… **Scale applications** vá»›i ReplicaSets
- âœ… **Troubleshoot** ReplicaSet issues

---

## ğŸ“¦ ReplicaSet LÃ  GÃ¬?

### Äá»‹nh NghÄ©a

**ReplicaSet** = Controller Ä‘áº£m báº£o má»™t sá»‘ lÆ°á»£ng cá»‘ Ä‘á»‹nh Pod replicas luÃ´n cháº¡y trong cluster.

### Giáº£i ThÃ­ch Báº±ng VÃ­ Dá»¥

**ReplicaSet giá»‘ng nhÆ° Supervisor quáº£n lÃ½ ca lÃ m viá»‡c:**

```
ğŸ¢ NHÃ€HÃ€NG - CA SÃNG (ReplicaSet)

Quy Ä‘á»‹nh: LuÃ´n pháº£i cÃ³ 3 nhÃ¢n viÃªn phá»¥c vá»¥
Supervisor (ReplicaSet Controller) continuously checks:

Scenario 1: Äá»§ ngÆ°á»i
  Actual: 3 nhÃ¢n viÃªn âœ“
  Desired: 3 nhÃ¢n viÃªn âœ“
  â†’ No action needed

Scenario 2: Thiáº¿u ngÆ°á»i (1 ngÆ°á»i á»‘m)
  Actual: 2 nhÃ¢n viÃªn âœ—
  Desired: 3 nhÃ¢n viÃªn âœ“
  â†’ Supervisor gá»i thÃªm 1 ngÆ°á»i
  â†’ Actual: 3 nhÃ¢n viÃªn âœ“

Scenario 3: DÆ° ngÆ°á»i (4 ngÆ°á»i Ä‘áº¿n)
  Actual: 4 nhÃ¢n viÃªn âœ—
  Desired: 3 nhÃ¢n viÃªn âœ“
  â†’ Supervisor cho 1 ngÆ°á»i vá»
  â†’ Actual: 3 nhÃ¢n viÃªn âœ“
```

### ReplicaSet trong K8s

```
REPLICASET = MAINTAIN REPLICA COUNT

Desired State: 3 Pod replicas
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ReplicaSet Controller             â”‚
â”‚  (Watching continuously)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Check  â”‚
    â”‚ Actual â”‚
    â”‚ vs     â”‚
    â”‚ Desiredâ”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚          â”‚
    â†“          â†“
Actual < 3    Actual > 3
Create Pod    Delete Pod
```

---

## ğŸ¤” Táº I SAO Cáº§n ReplicaSet?

### Váº¥n Äá» Chá»‰ DÃ¹ng Pods

**Problem: Pods khÃ´ng self-healing**

```bash
# Create a single Pod
kubectl run webapp --image=nginx

# Pod is running
$ kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
webapp   1/1     Running   0          1m

# Simulate crash (delete Pod)
$ kubectl delete pod webapp
pod "webapp" deleted

# Pod is gone forever!
$ kubectl get pods
No resources found in default namespace.

âŒ Application down!
âŒ Manual intervention required
âŒ No automatic recovery
âŒ Single point of failure
```

### Giáº£i PhÃ¡p: ReplicaSet

**Solution: Automatic recovery + scaling**

```bash
# Create ReplicaSet vá»›i 3 replicas
$ kubectl apply -f replicaset.yaml

# 3 Pods running
$ kubectl get pods
NAME         READY   STATUS    RESTARTS   AGE
webapp-abc   1/1     Running   0          1m
webapp-def   1/1     Running   0          1m
webapp-ghi   1/1     Running   0          1m

# Simulate crash (delete 1 Pod)
$ kubectl delete pod webapp-abc
pod "webapp-abc" deleted

# ReplicaSet immediately creates replacement!
$ kubectl get pods
NAME         READY   STATUS    RESTARTS   AGE
webapp-def   1/1     Running   0          2m
webapp-ghi   1/1     Running   0          2m
webapp-jkl   1/1     Running   0          5s  â† NEW!

âœ“ Automatic recovery!
âœ“ Always 3 Pods running
âœ“ High availability
âœ“ No manual intervention
```

---

## ğŸ—ï¸ ReplicaSet Components

### ReplicaSet Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            REPLICASET                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Metadata:                                      â”‚
â”‚  â”œâ”€ Name: webapp-rs                            â”‚
â”‚  â”œâ”€ Namespace: default                         â”‚
â”‚  â””â”€ Labels: {app: webapp}                      â”‚
â”‚                                                 â”‚
â”‚  Spec:                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. replicas: 3                          â”‚  â”‚
â”‚  â”‚     How many Pods to maintain            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. selector:                            â”‚  â”‚
â”‚  â”‚     matchLabels:                         â”‚  â”‚
â”‚  â”‚       app: webapp                        â”‚  â”‚
â”‚  â”‚     How to find Pods to manage           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. template:                            â”‚  â”‚
â”‚  â”‚     metadata:                            â”‚  â”‚
â”‚  â”‚       labels:                            â”‚  â”‚
â”‚  â”‚         app: webapp                      â”‚  â”‚
â”‚  â”‚     spec:                                â”‚  â”‚
â”‚  â”‚       containers: [...]                  â”‚  â”‚
â”‚  â”‚     Pod template for creating new Pods   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ manages â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PODS (created from template)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Pod 1   â”‚  â”‚ Pod 2   â”‚  â”‚ Pod 3   â”‚        â”‚
â”‚  â”‚app:     â”‚  â”‚app:     â”‚  â”‚app:     â”‚        â”‚
â”‚  â”‚webapp   â”‚  â”‚webapp   â”‚  â”‚webapp   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ ReplicaSet YAML

### Basic Example

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-rs
  labels:
    app: webapp
    tier: frontend
spec:
  # 1. Sá»‘ lÆ°á»£ng Pods mong muá»‘n
  replicas: 3
  
  # 2. Selector: TÃ¬m Pods Ä‘á»ƒ manage
  selector:
    matchLabels:
      app: webapp
      tier: frontend
  
  # 3. Template: Táº¡o Pods má»›i
  template:
    metadata:
      labels:
        app: webapp        # MUST match selector!
        tier: frontend     # MUST match selector!
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
```

### YAML Breakdown

**1. replicas**
```yaml
spec:
  replicas: 3

Meaning: "TÃ´i muá»‘n 3 Pods"
ReplicaSet sáº½ ensure luÃ´n cÃ³ exactly 3 Pods
```

**2. selector**
```yaml
spec:
  selector:
    matchLabels:
      app: webapp
      tier: frontend

Meaning: "Manage Pods cÃ³ labels nÃ y"
ReplicaSet counts Pods vá»›i matching labels
```

**3. template**
```yaml
spec:
  template:
    metadata:
      labels:
        app: webapp
        tier: frontend
    spec:
      containers: [...]

Meaning: "Template Ä‘á»ƒ táº¡o Pods má»›i"
When need new Pod â†’ Create from this template
```

**âš ï¸ CRITICAL:** Template labels MUST match selector!

```yaml
# GOOD âœ“
selector:
  matchLabels:
    app: webapp
template:
  metadata:
    labels:
      app: webapp  # Matches!

# BAD âœ—
selector:
  matchLabels:
    app: webapp
template:
  metadata:
    labels:
      app: api  # Mismatch! Will fail!
```

---

## ğŸ”„ ReplicaSet Reconciliation Loop

### ReplicaSet Hoáº¡t Äá»™ng NhÆ° Tháº¿ NÃ o

**VÃ²ng láº·p liÃªn tá»¥c:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VÃ’NG Láº¶P REPLICASET CONTROLLER             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Láº§n láº·p (má»—i 30s default):

1. Láº¤Y desired replicas
   â†“
   replicas: 3

2. Äáº¾M Pods hiá»‡n táº¡i (matching selector)
   â†“
   kubectl get pods -l app=webapp
   â†“
   TÃ¬m tháº¥y: 2 Pods

3. SO SÃNH desired vs actual
   â†“
   Desired: 3
   Actual: 2
   ChÃªnh lá»‡ch: -1 (cáº§n thÃªm 1 Pod)

4. HÃ€NH Äá»˜NG
   â†“
   Táº¡o 1 Pod tá»« template

5. CHá»œ Pod Running
   â†“
   Pod created â†’ Pending â†’ Running

6. Láº¶P Láº I
   â†“
   Láº§n láº·p tiáº¿p:
   Desired: 3
   Actual: 3
   â†’ KhÃ´ng cáº§n action âœ“

VÃ²ng láº·p tiáº¿p tá»¥c mÃ£i mÃ£i...
```

### Example Scenarios

**Scenario 1: Pod Crashes**

```
Initial state:
Desired: 3
Actual: 3 Pods running
âœ“ In sync

--- Pod 2 crashes ---

ReplicaSet detects (next loop):
Desired: 3
Actual: 2 Pods (Pod 1, Pod 3)
Difference: -1

Action:
â†’ Create 1 new Pod (Pod 4)

Final state:
Actual: 3 Pods (Pod 1, Pod 3, Pod 4)
âœ“ Recovered automatically!
```

**Scenario 2: Manual Scale Up**

```
Initial state:
Desired: 3
Actual: 3 Pods
âœ“ In sync

--- User scales to 5 ---
kubectl scale rs webapp-rs --replicas=5

ReplicaSet detects:
Desired: 5 (updated)
Actual: 3 Pods
Difference: -2

Action:
â†’ Create 2 new Pods

Final state:
Actual: 5 Pods
âœ“ Scaled up!
```

**Scenario 3: Manual Scale Down**

```
Initial state:
Desired: 5
Actual: 5 Pods
âœ“ In sync

--- User scales to 2 ---
kubectl scale rs webapp-rs --replicas=2

ReplicaSet detects:
Desired: 2 (updated)
Actual: 5 Pods
Difference: +3 (too many)

Action:
â†’ Delete 3 Pods (oldest first, by default)

Final state:
Actual: 2 Pods
âœ“ Scaled down!
```

---

## ğŸ® Hands-On: Working vá»›i ReplicaSets

### Create ReplicaSet

**Method 1: YAML file (Recommended)**

```yaml
# replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
```

```bash
# Create
kubectl apply -f replicaset.yaml

# Output:
# replicaset.apps/nginx-rs created

# Verify
kubectl get replicaset
# or shorter
kubectl get rs

# Output:
# NAME       DESIRED   CURRENT   READY   AGE
# nginx-rs   3         3         3       30s
```

**Method 2: kubectl create (less common)**

```bash
# Create ReplicaSet imperatively
kubectl create -f replicaset.yaml

# Difference: create vs apply
# create: Fails if exists
# apply: Updates if exists (declarative, recommended)
```

---

### Get ReplicaSets

```bash
# List all ReplicaSets
kubectl get rs

# Output:
# NAME       DESIRED   CURRENT   READY   AGE
# nginx-rs   3         3         3       5m

# With more details
kubectl get rs -o wide

# Output:
# NAME       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR
# nginx-rs   3         3         3       5m    nginx        nginx:1.21   app=nginx

# Show labels
kubectl get rs --show-labels
```

---

### Describe ReplicaSet

```bash
# Detailed information
kubectl describe rs nginx-rs

# Output (sample):
Name:         nginx-rs
Namespace:    default
Selector:     app=nginx
Labels:       app=nginx
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.21
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  2m    replicaset-controller  Created pod: nginx-rs-abc12
  Normal  SuccessfulCreate  2m    replicaset-controller  Created pod: nginx-rs-def34
  Normal  SuccessfulCreate  2m    replicaset-controller  Created pod: nginx-rs-ghi56
```

---

### List Pods Managed by ReplicaSet

```bash
# Get Pods vá»›i label selector
kubectl get pods -l app=nginx

# Output:
# NAME             READY   STATUS    RESTARTS   AGE
# nginx-rs-abc12   1/1     Running   0          5m
# nginx-rs-def34   1/1     Running   0          5m
# nginx-rs-ghi56   1/1     Running   0          5m

# Show owner reference (which ReplicaSet owns Pod)
kubectl get pods nginx-rs-abc12 -o yaml | grep -A 5 ownerReferences

# Output:
# ownerReferences:
# - apiVersion: apps/v1
#   kind: ReplicaSet
#   name: nginx-rs
#   uid: 12345678-1234-1234-1234-123456789012
```

---

### Scale ReplicaSet

**Method 1: kubectl scale**

```bash
# Scale up to 5 replicas
kubectl scale rs nginx-rs --replicas=5

# Output:
# replicaset.apps/nginx-rs scaled

# Verify
kubectl get rs nginx-rs

# Output:
# NAME       DESIRED   CURRENT   READY   AGE
# nginx-rs   5         5         5       10m

# Get Pods (now 5)
kubectl get pods -l app=nginx
```

**Method 2: Edit YAML**

```bash
# Edit ReplicaSet
kubectl edit rs nginx-rs

# In editor, change:
# spec:
#   replicas: 3  â†’  replicas: 7

# Save and exit
# ReplicaSet automatically scales to 7!

# Verify
kubectl get rs nginx-rs
# DESIRED: 7
```

**Method 3: Update YAML file vÃ  apply**

```yaml
# replicaset.yaml
spec:
  replicas: 10  # Changed from 3
```

```bash
# Apply changes
kubectl apply -f replicaset.yaml

# ReplicaSet scales to 10
kubectl get rs nginx-rs
```

---

### Test Self-Healing

```bash
# Get current Pods
kubectl get pods -l app=nginx

# Output:
# NAME             READY   STATUS    RESTARTS   AGE
# nginx-rs-abc12   1/1     Running   0          5m
# nginx-rs-def34   1/1     Running   0          5m
# nginx-rs-ghi56   1/1     Running   0          5m

# Delete one Pod
kubectl delete pod nginx-rs-abc12

# Immediately check
kubectl get pods -l app=nginx

# Output: ReplicaSet created new Pod!
# NAME             READY   STATUS    RESTARTS   AGE
# nginx-rs-def34   1/1     Running   0          5m
# nginx-rs-ghi56   1/1     Running   0          5m
# nginx-rs-jkl78   1/1     Running   0          2s  â† NEW!

# Always 3 Pods maintained!
```

---

### Delete ReplicaSet

**Option 1: Delete ReplicaSet AND Pods**

```bash
# Delete ReplicaSet (default: deletes Pods too)
kubectl delete rs nginx-rs

# Output:
# replicaset.apps "nginx-rs" deleted

# Pods are also deleted
kubectl get pods -l app=nginx
# No resources found
```

**Option 2: Delete ReplicaSet but KEEP Pods**

```bash
# Delete ReplicaSet without deleting Pods
kubectl delete rs nginx-rs --cascade=orphan

# ReplicaSet deleted, but Pods remain!
kubectl get rs
# No resources found

kubectl get pods -l app=nginx
# NAME             READY   STATUS    RESTARTS   AGE
# nginx-rs-abc12   1/1     Running   0          10m
# nginx-rs-def34   1/1     Running   0          10m
# nginx-rs-ghi56   1/1     Running   0          10m

# Pods are now orphaned (no longer managed)
# Must manually delete if needed
```

---

## âš ï¸ ReplicaSet Label Matching

### Selector Hoáº¡t Äá»™ng NhÆ° Tháº¿ NÃ o

**ReplicaSet counts ALL Pods vá»›i matching labels trong namespace**

```bash
# Create ReplicaSet vá»›i 3 replicas
kubectl apply -f replicaset.yaml

# ReplicaSet creates 3 Pods
kubectl get pods -l app=nginx
# 3 Pods

# Manually create Pod vá»›i SAME labels
kubectl run manual-pod --image=nginx --labels="app=nginx"

# ReplicaSet sees 4 Pods vá»›i app=nginx!
kubectl get pods -l app=nginx
# 4 Pods

# ReplicaSet detects:
# Desired: 3
# Actual: 4
# Difference: +1 (too many!)

# ReplicaSet DELETES 1 Pod (random selection)
# Could be manual-pod or one of ReplicaSet Pods!

kubectl get pods -l app=nginx
# Back to 3 Pods
```

**âš ï¸ WARNING:** Don't manually create Pods vá»›i labels matching ReplicaSet selector!

---

## ğŸ”— ReplicaSet vs Deployment

### Relationship

```
DEPLOYMENT
    â†“ creates vÃ  manages
REPLICASET
    â†“ creates vÃ  manages
PODS

Trong thá»±c táº¿:
âŒ DON'T use ReplicaSets directly
âœ“ USE Deployments instead!
```

### Táº¡i Sao DÃ¹ng Deployments?

```
ReplicaSet limitations:
âŒ No rolling updates
âŒ No rollback capability
âŒ No update strategies
âŒ Manual version management

Deployment features:
âœ“ Rolling updates
âœ“ Rollback to previous versions
âœ“ Update strategies (RollingUpdate, Recreate)
âœ“ Revision history
âœ“ Declarative updates

Deployment = ReplicaSet + Update Management
```

**Example:**

```yaml
# Create Deployment (recommended)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
```

```bash
# Deployment creates ReplicaSet automatically
kubectl apply -f deployment.yaml

# Check Deployment
kubectl get deployments
# NAME     READY   UP-TO-DATE   AVAILABLE   AGE
# webapp   3/3     3            3           1m

# Check ReplicaSet (created by Deployment)
kubectl get rs
# NAME                DESIRED   CURRENT   READY   AGE
# webapp-7d4b7c9d8f   3         3         3       1m

# Check Pods (created by ReplicaSet)
kubectl get pods
# NAME                      READY   STATUS    RESTARTS   AGE
# webapp-7d4b7c9d8f-abc12   1/1     Running   0          1m
# webapp-7d4b7c9d8f-def34   1/1     Running   0          1m
# webapp-7d4b7c9d8f-ghi56   1/1     Running   0          1m
```

---

## ğŸ› Troubleshooting ReplicaSets

### Issue 1: Pods Not Created

```bash
$ kubectl get rs
NAME       DESIRED   CURRENT   READY   AGE
webapp-rs  3         0         0       5m

# No Pods created!

# Describe ReplicaSet
$ kubectl describe rs webapp-rs

# Events might show:
# Error creating: Pod "webapp-rs-abc12" is invalid: 
# spec.containers[0].image: Required value

# Fix: Check Pod template in ReplicaSet
# - Image name correct?
# - Resource limits valid?
# - Volumes exist?
```

---

### Issue 2: Desired != Current

```bash
$ kubectl get rs
NAME       DESIRED   CURRENT   READY   AGE
webapp-rs  5         3         3       5m

# Stuck at 3, can't reach 5

# Possible causes:
# 1. Resource limits (cluster full)
kubectl describe nodes | grep -A 5 "Allocated resources"

# 2. Image pull errors
kubectl get pods -l app=webapp
# Look for ImagePullBackOff

# 3. Node selector mismatch
kubectl describe rs webapp-rs | grep "Node Selector"
```

---

### Issue 3: Pods Constantly Restarting

```bash
$ kubectl get pods -l app=webapp
NAME             READY   STATUS             RESTARTS   AGE
webapp-rs-abc    0/1     CrashLoopBackOff   5          5m

# ReplicaSet keeps creating Pods, they keep crashing

# Debug:
kubectl logs webapp-rs-abc
kubectl logs webapp-rs-abc --previous

kubectl describe pod webapp-rs-abc
# Check: Last State, Exit Code, Reason

# Common causes:
# - Application error
# - Missing environment variables
# - OOMKilled (increase memory limits)
```

---

### Issue 4: Too Many Pods

```bash
$ kubectl get rs
NAME       DESIRED   CURRENT   READY   AGE
webapp-rs  3         3         3       5m

$ kubectl get pods -l app=webapp
# Shows 6 Pods!?

# Reason: Multiple ReplicaSets vá»›i same selector!
kubectl get rs --show-labels
# Check if multiple ReplicaSets have same selector

# Or: Orphaned Pods vá»›i matching labels
kubectl get pods -l app=webapp -o yaml | grep ownerReferences
# Pods without owner = orphaned

# Fix: Delete orphaned Pods or redundant ReplicaSets
```

---

## ğŸ“ Kiá»ƒm Tra Hiá»ƒu Biáº¿t

**1. ReplicaSet lÃ m gÃ¬ khi Pod crashes?**
<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

1. ReplicaSet controller detects Pod count < desired
2. Creates new Pod from template
3. Waits for Pod to be Running
4. Count matches desired â†’ Done

Automatic, no human intervention!
</details>

**2. Selector labels vÃ  template labels pháº£i match?**
<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

YES! MUST match!

```yaml
# Correct
selector:
  matchLabels:
    app: webapp
template:
  metadata:
    labels:
      app: webapp  # Matches selector

# Wrong
selector:
  matchLabels:
    app: webapp
template:
  metadata:
    labels:
      app: api  # Doesn't match! Error!
```

If don't match â†’ ReplicaSet creation fails with validation error.
</details>

**3. NÃªn dÃ¹ng ReplicaSet hay Deployment?**
<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

**USE DEPLOYMENT!**

Deployment = ReplicaSet + Updates + Rollbacks

ReplicaSet alone: Only maintains replica count
Deployment: Replica count + Rolling updates + Rollback + History

Exception: Chá»‰ dÃ¹ng ReplicaSet directly if you need low-level control vÃ  manage updates manually.
</details>

---

## ğŸ’ª BÃ i Táº­p Thá»±c HÃ nh

### BÃ i 1: Create vÃ  Scale ReplicaSet

```yaml
# exercise-rs.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-rs
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

```bash
# 1. Create ReplicaSet
kubectl apply -f exercise-rs.yaml

# 2. Verify
kubectl get rs
kubectl get pods -l app=webapp

# 3. Scale to 5
kubectl scale rs webapp-rs --replicas=5

# 4. Verify
kubectl get pods -l app=webapp
# Should see 5 Pods

# 5. Scale down to 1
kubectl scale rs webapp-rs --replicas=1

# 6. Verify (4 Pods terminated)
kubectl get pods -l app=webapp

# 7. Cleanup
kubectl delete rs webapp-rs
```

---

### BÃ i 2: Test Self-Healing

```bash
# Use ReplicaSet from BÃ i 1 (3 replicas)
kubectl scale rs webapp-rs --replicas=3

# Get Pod names
kubectl get pods -l app=webapp

# Pick one Pod and delete it
kubectl delete pod webapp-rs-<random-hash>

# Immediately watch
kubectl get pods -l app=webapp -w

# You'll see:
# - Deleted Pod: Terminating
# - New Pod: ContainerCreating â†’ Running

# Always 3 Pods maintained!
```

---

### BÃ i 3: Label Matching Experiment

```bash
# 1. Create ReplicaSet vá»›i 2 replicas
kubectl apply -f exercise-rs.yaml

# 2. Verify 2 Pods created
kubectl get pods -l app=webapp

# 3. Manually create Pod vá»›i SAME labels
kubectl run manual-pod --image=nginx --labels="app=webapp"

# 4. Check Pods (3 Pods!)
kubectl get pods -l app=webapp

# 5. Wait a few seconds...
# ReplicaSet detects 3 > 2, deletes 1 Pod!

kubectl get pods -l app=webapp
# Back to 2 Pods (manual-pod might be deleted!)

# 6. Cleanup
kubectl delete rs webapp-rs
kubectl delete pod manual-pod --ignore-not-found
```

---

## ğŸ¯ Key Takeaways

1. **ReplicaSet = Maintain Replica Count**
   - Ensures desired number of Pods
   - Automatic self-healing
   - Continuous reconciliation loop

2. **Three Key Components**
   - replicas: Desired count
   - selector: How to find Pods
   - template: How to create Pods

3. **Selector = Template Labels**
   - MUST match!
   - Validation error if mismatch

4. **Self-Healing Automatic**
   - Pod crashes â†’ New Pod created
   - No manual intervention
   - High availability

5. **Use Deployments Instead**
   - ReplicaSet = Low-level
   - Deployment = ReplicaSet + Updates
   - Production: Always use Deployments

---

## ğŸš€ Tiáº¿p Theo

ReplicaSet náº¯m rá»“i! Next: Deployment - Production-ready workload management!

**Next:** [4.2. Deployment â†’](./02-deployment.md)

---

[â¬…ï¸ Pháº§n 3: Core Concepts](../03-core-concepts/README.md) | [ğŸ  Má»¥c Lá»¥c](../README.md) | [ğŸ“‚ Pháº§n 4: Workloads](./README.md) | [â¡ï¸ 4.2. Deployment](./02-deployment.md)
