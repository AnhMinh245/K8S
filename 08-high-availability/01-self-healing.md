# 8.1. Self-Healing - Tá»± Phá»¥c Há»“i

> Kubernetes tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  sá»­a lá»—i mÃ  khÃ´ng cáº§n can thiá»‡p thá»§ cÃ´ng

---

## ğŸ“– Má»¥c Lá»¥c

1. [Self-Healing lÃ  gÃ¬?](#-self-healing-lÃ -gÃ¬)
2. [Control Loop - VÃ²ng Láº·p Äiá»u Khiá»ƒn](#-control-loop---vÃ²ng-láº·p-Ä‘iá»u-khiá»ƒn)
3. [Scenario 1: Container Crash](#-scenario-1-container-crash)
4. [Scenario 2: Health Check Failed](#-scenario-2-health-check-failed)
5. [Scenario 3: Pod Deleted](#-scenario-3-pod-deleted)
6. [Scenario 4: Node Failure](#-scenario-4-node-failure)
7. [Scenario 5: OOMKilled](#-scenario-5-oomkilled)
8. [Scenario 6: Disk Pressure](#-scenario-6-disk-pressure)
9. [RestartPolicy](#-restartpolicy)
10. [Hands-on Labs](#-hands-on-labs)
11. [Troubleshooting](#-troubleshooting)
12. [Best Practices](#-best-practices)

---

## ğŸ¤” Self-Healing lÃ  gÃ¬?

### Äá»‹nh nghÄ©a

**Self-Healing** lÃ  kháº£ nÄƒng cá»§a Kubernetes tá»± Ä‘á»™ng:
- ğŸ” **Detect:** PhÃ¡t hiá»‡n lá»—i (container crash, node down, health check fail)
- ğŸ”§ **Fix:** Tá»± Ä‘á»™ng sá»­a lá»—i (restart container, recreate Pod, reschedule)
- âœ… **Verify:** Äáº£m báº£o tráº¡ng thÃ¡i mong muá»‘n Ä‘áº¡t Ä‘Æ°á»£c

### VÃ­ dá»¥ thá»±c táº¿

**âŒ KhÃ´ng cÃ³ Self-Healing (Traditional servers):**
```
3:00 AM - Server crash
3:05 AM - Monitoring alert
3:10 AM - On-call engineer wakes up â˜•
3:20 AM - SSH vÃ o server
3:30 AM - Debug logs
3:45 AM - Restart service
4:00 AM - Service back online

Total downtime: 60 minutes ğŸ”¥
Engineer sleep lost: 1 night ğŸ˜´
```

**âœ… CÃ³ Self-Healing (Kubernetes):**
```
3:00 AM - Container crash
3:00 AM - kubelet detects (instant!)
3:00 AM - Restart container (automatic!)
3:01 AM - Service back online âœ…

Total downtime: 1 minute ğŸ‰
Engineer sleep lost: 0 nights ğŸ˜´âœ…
```

### So sÃ¡nh

| Aspect | Traditional | Kubernetes Self-Healing |
|--------|-------------|-------------------------|
| **Detection** | Manual monitoring | Automatic (kubelet, controllers) |
| **Response time** | Minutes to hours | Seconds |
| **Human intervention** | Required | Not required |
| **Consistency** | Depends on engineer | Always same process |
| **Cost** | On-call engineers | Free (built-in) |

---

## ğŸ”„ Control Loop - VÃ²ng Láº·p Äiá»u Khiá»ƒn

### NguyÃªn lÃ½ cÆ¡ báº£n

**Má»i controller trong K8s Ä‘á»u cháº¡y vÃ²ng láº·p nÃ y:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CONTROL LOOP (VÃ²ng láº·p)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

loop forever {
  1ï¸âƒ£  desired_state = read_from_api_server()
      // "TÃ´i muá»‘n 3 replicas"
  
  2ï¸âƒ£  current_state = observe_reality()
      // "Hiá»‡n táº¡i chá»‰ cÃ³ 2 Pods running"
  
  3ï¸âƒ£  if (current_state != desired_state) {
        take_action_to_fix()
        // "Táº¡o thÃªm 1 Pod ná»¯a!"
      }
  
  4ï¸âƒ£  sleep(sync_interval)  // Default: 10s
}
```

### VÃ­ dá»¥: ReplicaSet Controller

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: web
spec:
  replicas: 3  # â† DESIRED STATE
```

**Timeline:**

```
00:00 - ReplicaSet created
        â”œâ”€â”€ Desired: 3 Pods
        â””â”€â”€ Current: 0 Pods
        
00:00 - Controller loop iteration #1
        â”œâ”€â”€ Diff detected: 0 != 3
        â””â”€â”€ Action: Create 3 Pods
        
00:05 - Pods created
        â”œâ”€â”€ Desired: 3 Pods
        â””â”€â”€ Current: 3 Pods âœ…
        
00:10 - Controller loop iteration #2
        â””â”€â”€ No diff, do nothing
        
01:00 - Someone deletes 1 Pod!
        â”œâ”€â”€ Desired: 3 Pods
        â””â”€â”€ Current: 2 Pods âŒ
        
01:00 - Controller loop iteration #n
        â”œâ”€â”€ Diff detected: 2 != 3
        â””â”€â”€ Action: Create 1 Pod
        
01:05 - New Pod created
        â”œâ”€â”€ Desired: 3 Pods
        â””â”€â”€ Current: 3 Pods âœ…
```

**Key insight:**
```
Self-Healing = Control Loop cháº¡y liÃªn tá»¥c + API Server lÃ m source of truth
```

---

## ğŸ”¥ Scenario 1: Container Crash

### Ká»‹ch báº£n

**Application cÃ³ bug, crash khi nháº­n request Ä‘áº·c biá»‡t:**

```go
// Buggy code
func handleRequest(r *Request) {
  data := r.Body
  result := process(data)  // â† Panic if data == nil!
  return result
}
```

### Timeline chi tiáº¿t

```
00:00:00 - Pod running (container ID: abc123)
           â””â”€â”€ Status: Running, Ready: True
           
00:00:15 - User sends malformed request
           â””â”€â”€ data == nil
           
00:00:15 - Application panics!
           â””â”€â”€ exit code: 1
           
00:00:15 - Container exits
           â”œâ”€â”€ kubelet detects immediately (waiting on container process)
           â””â”€â”€ Status: Running â†’ Terminated
           
00:00:15 - kubelet checks restartPolicy
           â””â”€â”€ restartPolicy: Always â†’ RESTART!
           
00:00:15 - kubelet starts new container (container ID: def456)
           â”œâ”€â”€ Pull image (if needed)
           â”œâ”€â”€ Create container
           â””â”€â”€ Start container
           
00:00:20 - Container running again
           â”œâ”€â”€ Status: Running, Ready: True
           â””â”€â”€ Restart count: 1
           
Total downtime: 5 seconds âœ…
```

### Workflow diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CONTAINER CRASH RECOVERY             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pod Running
    â†“
Container crashes (exit code != 0)
    â†“
kubelet detects container exit
    â†“
Check restartPolicy
    â”œâ”€ Always      â†’ Restart
    â”œâ”€ OnFailure   â†’ Restart (exit != 0)
    â””â”€ Never       â†’ Don't restart
    â†“
Pull image (if needed)
    â†“
Create new container
    â†“
Start container
    â†“
Run postStart hook (if configured)
    â†“
Health checks pass?
    â”œâ”€ YES â†’ Status: Running, Ready: True
    â””â”€ NO  â†’ Keep restarting (exponential backoff)
```

### Exponential Backoff

**Náº¿u container liÃªn tá»¥c crash:**

```
Crash #1 â†’ Wait 0s   â†’ Restart (immediate)
Crash #2 â†’ Wait 10s  â†’ Restart
Crash #3 â†’ Wait 20s  â†’ Restart
Crash #4 â†’ Wait 40s  â†’ Restart
Crash #5 â†’ Wait 80s  â†’ Restart
Crash #6 â†’ Wait 160s â†’ Restart
...
Max wait: 5 minutes (300s)
```

**Why?** TrÃ¡nh "thundering herd" problem:
```
100 Pods cÃ¹ng crash â†’ cÃ¹ng restart â†’ cÃ¹ng hit database
â†’ Database overwhelmed â†’ crash again â†’ infinite loop! ğŸ”¥
```

### Example

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: crashy-app
spec:
  containers:
  - name: app
    image: my-buggy-app:1.0
    restartPolicy: Always  # â† Tá»± Ä‘á»™ng restart khi crash
```

```bash
# Deploy
kubectl apply -f crashy-app.yaml

# Simulate crash
kubectl exec crashy-app -- killall -9 app

# Watch restart
kubectl get pod crashy-app -w
```

**Output:**
```
NAME         READY   STATUS    RESTARTS   AGE
crashy-app   1/1     Running   0          1m
crashy-app   0/1     Error     0          1m5s   â† Container crashed
crashy-app   1/1     Running   1          1m10s  â† Restarted! (RESTARTS=1)
```

---

## ğŸ’Š Scenario 2: Health Check Failed

### Ká»‹ch báº£n

**Application stuck (deadlock, infinite loop) nhÆ°ng process váº«n running:**

```python
# App stuck in infinite loop
while True:
    try:
        result = database.query("SELECT * FROM huge_table")
        process(result)  # â† Takes 10 minutes!
    except:
        pass  # â† Swallow errors, keep "running"

# Container process: Running âœ…
# Application: Completely stuck âŒ
# Health endpoint: Timeout âŒ
```

### Timeline

```
00:00 - Pod starts, app healthy
        â””â”€â”€ Liveness probe: GET /health â†’ 200 OK âœ…
        
00:10 - App enters deadlock
        â”œâ”€â”€ Process: Still running
        â””â”€â”€ Liveness probe: GET /health â†’ Timeout âŒ
        
00:20 - Liveness probe fails (attempt 1/3)
        â””â”€â”€ kubelet: "Hmm, might be temporary..."
        
00:30 - Liveness probe fails (attempt 2/3)
        â””â”€â”€ kubelet: "Still failing..."
        
00:40 - Liveness probe fails (attempt 3/3)
        â””â”€â”€ kubelet: "OK, killing container!"
        
00:40 - kubelet kills container
        â””â”€â”€ SIGTERM â†’ wait 30s â†’ SIGKILL
        
00:45 - Container terminated, restart
        â””â”€â”€ New container starts
        
00:50 - App healthy again
        â””â”€â”€ Liveness probe: GET /health â†’ 200 OK âœ…
        
Total stuck time: 50 seconds âœ…
```

### Liveness Probe config

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  containers:
  - name: app
    image: my-web-app:1.0
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 10  # â† Wait 10s after start
      periodSeconds: 10        # â† Check every 10s
      timeoutSeconds: 5        # â† 5s timeout
      failureThreshold: 3      # â† Fail 3 times â†’ restart
      successThreshold: 1      # â† Success 1 time â†’ healthy
```

**What happens:**
```
initialDelaySeconds (10s): Wait for app to start
    â†“
First probe at 10s â†’ Success
    â†“
Second probe at 20s â†’ Success
    â†“
Third probe at 30s â†’ FAIL (timeout after 5s)
    â†“
Fourth probe at 40s â†’ FAIL (attempt 2/3)
    â†“
Fifth probe at 50s â†’ FAIL (attempt 3/3)
    â†“
Restart container! ğŸ”„
```

---

## ğŸ—‘ï¸ Scenario 3: Pod Deleted

### Ká»‹ch báº£n

**Admin vÃ´ tÃ¬nh xÃ³a Pod:**

```bash
kubectl delete pod web-app-12345
```

**Hoáº·c Node drain:**

```bash
kubectl drain node-1 --ignore-daemonsets
```

### Timeline

```
00:00 - Deployment: web-app, replicas: 3
        â”œâ”€â”€ Pod web-app-aaa (Node 1)
        â”œâ”€â”€ Pod web-app-bbb (Node 2)
        â””â”€â”€ Pod web-app-ccc (Node 3)
        
00:00 - Admin deletes Pod web-app-aaa
        â””â”€â”€ kubectl delete pod web-app-aaa
        
00:00 - API Server marks Pod for deletion
        â””â”€â”€ deletionTimestamp set
        
00:00 - kubelet on Node 1 detects
        â”œâ”€â”€ Send SIGTERM to container
        â”œâ”€â”€ Wait terminationGracePeriodSeconds (default 30s)
        â””â”€â”€ Send SIGKILL if still running
        
00:01 - Pod deleted from API Server
        â””â”€â”€ Current replicas: 2 (bbb, ccc)
        
00:01 - ReplicaSet Controller wakes up
        â”œâ”€â”€ Desired: 3 replicas
        â”œâ”€â”€ Current: 2 replicas
        â””â”€â”€ Diff: Need 1 more Pod!
        
00:01 - Controller creates new Pod
        â””â”€â”€ Pod web-app-ddd created
        
00:02 - Scheduler assigns to Node
        â””â”€â”€ Pod web-app-ddd â†’ Node 1
        
00:03 - kubelet on Node 1 starts Pod
        â”œâ”€â”€ Pull image
        â”œâ”€â”€ Create container
        â””â”€â”€ Start container
        
00:08 - Pod web-app-ddd ready
        â””â”€â”€ Current replicas: 3 (bbb, ccc, ddd) âœ…
        
Total recovery time: 8 seconds âœ…
```

### Graceful Shutdown

**Äá»ƒ Pod shutdown sáº¡ch sáº½ (save data, close connections):**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  containers:
  - name: app
    image: my-app:1.0
    lifecycle:
      preStop:
        exec:
          command:
          - /bin/sh
          - -c
          - |
            echo "Shutting down gracefully..."
            # Close connections
            curl -X POST localhost:8080/shutdown
            # Wait for requests to finish
            sleep 10
  terminationGracePeriodSeconds: 30  # â† Max wait time
```

**Workflow:**
```
kubectl delete pod web-app
    â†“
1. Pod.status.phase = Terminating
2. Removed from Service endpoints (no new traffic)
3. preStop hook executed
4. Wait up to terminationGracePeriodSeconds (30s)
5. SIGTERM sent to container
6. Wait 30s
7. SIGKILL if still running (force kill)
8. Pod deleted
```

---

## ğŸ–¥ï¸ Scenario 4: Node Failure

### Ká»‹ch báº£n

**Node bá»‹ máº¥t Ä‘iá»‡n/network:**

```
Data center power outage
    â†“
Node 2 shutdown
    â†“
All Pods on Node 2 unreachable
```

### Timeline chi tiáº¿t

```
00:00 - Cluster healthy
        â”œâ”€â”€ Node 1: 10 Pods
        â”œâ”€â”€ Node 2: 10 Pods
        â””â”€â”€ Node 3: 10 Pods
        
00:00 - Node 2 goes down (power outage)
        â””â”€â”€ kubelet stops sending heartbeats
        
00:10 - Node Controller detects missing heartbeats
        â””â”€â”€ Last heartbeat: 10s ago
        
00:40 - Node marked NotReady (after 40s)
        â””â”€â”€ Node 2: Ready â†’ NotReady
        
00:40 - Node Controller waits (default: 5 minutes)
        â””â”€â”€ "Maybe temporary network issue..."
        
05:40 - Still NotReady, start eviction
        â””â”€â”€ Node Controller marks all Pods for deletion
        
05:40 - ReplicaSet Controllers detect Pod deletion
        â”œâ”€â”€ Deployment A: 3 replicas, now 2
        â”œâ”€â”€ Deployment B: 5 replicas, now 3
        â””â”€â”€ ...
        
05:40 - Controllers create replacement Pods
        â””â”€â”€ Schedule to healthy Nodes (1 & 3)
        
05:45 - Scheduler assigns new Pods
        â”œâ”€â”€ Node 1: +5 Pods
        â””â”€â”€ Node 3: +5 Pods
        
05:50 - kubelet starts Pods on Node 1 & 3
        â””â”€â”€ Pull images, create containers
        
06:00 - All Pods running on healthy Nodes âœ…
        â”œâ”€â”€ Node 1: 15 Pods
        â””â”€â”€ Node 3: 15 Pods
        
Total downtime: 6 minutes
```

### Pod Eviction Settings

**TÃ¹y chá»‰nh thá»i gian eviction:**

```bash
# kube-controller-manager flags
--pod-eviction-timeout=5m0s  # â† Default: 5 minutes
--node-monitor-grace-period=40s  # â† Mark NotReady after 40s
--node-monitor-period=5s  # â† Check every 5s
```

**Faster eviction cho dev clusters:**
```bash
--pod-eviction-timeout=1m0s  # â† 1 minute (faster recovery)
```

**Slower eviction cho production:**
```bash
--pod-eviction-timeout=10m0s  # â† 10 minutes (avoid false positives)
```

### PodDisruptionBudget

**Äáº£m báº£o availability trong quÃ¡ trÃ¬nh eviction:**

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-app-pdb
spec:
  minAvailable: 2  # â† Always keep 2 Pods running
  selector:
    matchLabels:
      app: web-app
```

**What happens:**
```
Deployment: 3 replicas
Node failure â†’ 1 Pod down
    â†“
Current: 2 replicas (meets minAvailable=2)
    â†“
Controller creates 1 new Pod
    â†“
New Pod starts â†’ Current: 3 replicas âœ…
```

---

## ğŸ’€ Scenario 5: OOMKilled

### Ká»‹ch báº£n

**Container sá»­ dá»¥ng quÃ¡ nhiá»u memory:**

```python
# Memory leak
data = []
while True:
    data.append("x" * 1024 * 1024)  # â† 1MB per iteration
    # Forget to clear data â†’ memory grows infinitely!
```

### Timeline

```
00:00 - Pod starts
        â””â”€â”€ Memory limit: 512Mi
        
00:10 - App uses 256Mi (50%)
        â””â”€â”€ Normal
        
00:30 - Memory leak!
        â””â”€â”€ App uses 400Mi (78%)
        
00:45 - App uses 500Mi (98%)
        â””â”€â”€ Linux OOM killer watching...
        
00:50 - App tries to allocate more (520Mi > 512Mi limit)
        â””â”€â”€ OOM killer: "STOP RIGHT THERE!" ğŸ’€
        
00:50 - Container killed (SIGKILL)
        â”œâ”€â”€ Reason: OOMKilled
        â””â”€â”€ Exit code: 137
        
00:50 - kubelet restarts container
        â””â”€â”€ Fresh start with 0Mi memory
        
00:55 - Container running again
        â””â”€â”€ Memory: 100Mi (normal)
        
If memory leak persists â†’ crash again!
```

### Memory limits

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
spec:
  containers:
  - name: app
    image: my-app:1.0
    resources:
      requests:
        memory: "256Mi"  # â† Minimum guaranteed
      limits:
        memory: "512Mi"  # â† Maximum allowed (OOM if exceeded)
```

### Debug OOMKilled

```bash
# Check Pod status
kubectl get pod memory-demo
# NAME          READY   STATUS      RESTARTS   AGE
# memory-demo   0/1     OOMKilled   5          10m

# Describe Pod
kubectl describe pod memory-demo
# Last State:     Terminated
#   Reason:       OOMKilled
#   Exit Code:    137

# Check logs before crash
kubectl logs memory-demo --previous
```

**Fix:**
```yaml
resources:
  limits:
    memory: "1Gi"  # â† Increase limit
```

---

## ğŸ’¾ Scenario 6: Disk Pressure

### Ká»‹ch báº£n

**Node disk Ä‘áº§y:**

```
Node disk: 100GB
Used: 95GB (logs, images, temp files)
Available: 5GB (< 10% threshold)
```

### Timeline

```
00:00 - Node disk: 80GB/100GB (80%)
        â””â”€â”€ Normal
        
02:00 - Apps writing logs
        â””â”€â”€ 85GB/100GB (85%)
        
04:00 - More logs, images pulled
        â””â”€â”€ 90GB/100GB (90%)
        
06:00 - Critical! 95GB/100GB (95%)
        â””â”€â”€ kubelet detects disk pressure
        
06:00 - Node marked with condition
        â””â”€â”€ DiskPressure: True
        
06:00 - kubelet starts evicting Pods
        â”œâ”€â”€ Priority: BestEffort Pods first
        â”œâ”€â”€ Then: Burstable Pods
        â””â”€â”€ Last: Guaranteed Pods
        
06:05 - Evicted 5 Pods
        â””â”€â”€ Freed: 10GB disk space
        
06:05 - Disk: 85GB/100GB (85%)
        â””â”€â”€ DiskPressure: False
        
06:10 - Evicted Pods rescheduled
        â””â”€â”€ Scheduled to healthy Nodes âœ…
```

### Disk pressure thresholds

```bash
# kubelet flags
--eviction-hard=nodefs.available<10%  # â† Hard eviction at 10%
--eviction-soft=nodefs.available<15%  # â† Soft eviction at 15%
--eviction-soft-grace-period=nodefs.available=2m  # â† Wait 2min
```

### Prevention

**1. Log rotation:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  containers:
  - name: app
    image: my-app:1.0
    volumeMounts:
    - name: logs
      mountPath: /var/log
  volumes:
  - name: logs
    emptyDir:
      sizeLimit: 1Gi  # â† Limit log size
```

**2. Image garbage collection:**
```bash
# kubelet flags
--image-gc-high-threshold=85  # â† Start GC at 85% disk usage
--image-gc-low-threshold=80   # â† Stop GC at 80% disk usage
```

**3. Regular cleanup:**
```bash
# Delete unused images
kubectl run cleanup --image=alpine --rm -it --restart=Never -- sh -c "
  docker image prune -a -f
"
```

---

## ğŸ”„ï¸ RestartPolicy

### 3 Options

**1. Always (Default):**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  restartPolicy: Always  # â† Restart regardless of exit code
  containers:
  - name: app
    image: nginx
```

**Behavior:**
```
Exit code 0 (success) â†’ Restart
Exit code 1 (failure) â†’ Restart
OOMKilled â†’ Restart
SIGKILL â†’ Restart
```

**Use case:**
- âœ… Web servers (nginx, Apache)
- âœ… APIs (REST, gRPC)
- âœ… Long-running services

**2. OnFailure:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: batch-job
spec:
  restartPolicy: OnFailure  # â† Only restart on failure
  containers:
  - name: job
    image: my-batch-job:1.0
```

**Behavior:**
```
Exit code 0 (success) â†’ DON'T restart (job done!)
Exit code 1 (failure) â†’ Restart
```

**Use case:**
- âœ… Batch jobs (data processing, ETL)
- âœ… One-time tasks (database migration)

**3. Never:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-setup
spec:
  restartPolicy: Never  # â† Never restart
  containers:
  - name: setup
    image: my-init-script:1.0
```

**Behavior:**
```
Exit code 0 (success) â†’ DON'T restart
Exit code 1 (failure) â†’ DON'T restart
```

**Use case:**
- âœ… Init containers (setup, config)
- âœ… Debug Pods (troubleshooting)

### Comparison table

| RestartPolicy | Exit 0 | Exit 1 | Use case |
|---------------|--------|--------|----------|
| **Always** | Restart | Restart | Long-running services |
| **OnFailure** | âŒ | Restart | Batch jobs |
| **Never** | âŒ | âŒ | One-time tasks |

---

## ğŸ§ª Hands-on Labs

### Lab 1: Container Crash

```bash
# Create crashy Pod
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: crashy
spec:
  containers:
  - name: app
    image: alpine
    command: ["sh", "-c", "echo Starting...; sleep 10; exit 1"]
    # â† Crash after 10s
EOF

# Watch Pod
kubectl get pod crashy -w
```

**Expected:**
```
NAME     READY   STATUS    RESTARTS   AGE
crashy   1/1     Running   0          5s
crashy   0/1     Error     0          15s  â† Crashed!
crashy   1/1     Running   1          20s  â† Restarted!
crashy   0/1     Error     1          30s  â† Crashed again
crashy   1/1     Running   2          35s  â† Restarted again (RESTARTS=2)
```

**Check restart count:**
```bash
kubectl get pod crashy -o jsonpath='{.status.containerStatuses[0].restartCount}'
# Output: 2
```

### Lab 2: Liveness Probe

```bash
# Create unhealthy app
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: unhealthy
spec:
  containers:
  - name: app
    image: nginx
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy  # â† File doesn't exist!
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 2
EOF

# Watch Pod (will restart due to failed probe)
kubectl get pod unhealthy -w
```

**Expected:**
```
NAME        READY   STATUS    RESTARTS   AGE
unhealthy   1/1     Running   0          10s
unhealthy   1/1     Running   1          25s  â† Restarted! (probe failed)
```

**Fix by creating file:**
```bash
kubectl exec unhealthy -- touch /tmp/healthy
# Now probe succeeds! âœ…
```

### Lab 3: Pod Deletion Recovery

```bash
# Create Deployment
kubectl create deployment web --image=nginx --replicas=3

# Watch Pods
kubectl get pods -l app=web -w &

# Delete one Pod
POD=$(kubectl get pods -l app=web -o jsonpath='{.items[0].metadata.name}')
kubectl delete pod $POD

# Expected: New Pod created automatically!
```

**Output:**
```
NAME                   READY   STATUS    RESTARTS   AGE
web-12345-aaa          1/1     Running   0          1m
web-12345-bbb          1/1     Running   0          1m
web-12345-ccc          1/1     Running   0          1m
web-12345-aaa          1/1     Terminating   0      1m5s  â† Deleted
web-12345-ddd          0/1     Pending       0      0s    â† New Pod!
web-12345-ddd          1/1     Running       0      5s    â† Running!
```

### Lab 4: Simulate Node Failure

```bash
# Mark Node as unschedulable
kubectl cordon node-1

# Drain Node (evict Pods)
kubectl drain node-1 --ignore-daemonsets --delete-emptydir-data

# Watch Pods reschedule
kubectl get pods -o wide -w
```

**Expected:**
```
NAME      READY   STATUS    RESTARTS   AGE   NODE
web-aaa   1/1     Running   0          1m    node-1
web-aaa   1/1     Terminating  0       1m5s  node-1  â† Evicting
web-ddd   0/1     Pending      0       0s    <none>  â† New Pod
web-ddd   1/1     Running      0       5s    node-2  â† Rescheduled!
```

**Restore Node:**
```bash
kubectl uncordon node-1
```

---

## ğŸ”§ Troubleshooting

### Issue 1: Pod stuck in CrashLoopBackOff

**Symptoms:**
```bash
kubectl get pods
# NAME    READY   STATUS             RESTARTS   AGE
# web     0/1     CrashLoopBackOff   10         15m
```

**Meaning:**
- Container keeps crashing
- kubelet backs off before restarting (exponential backoff)

**Debug:**
```bash
# Check logs
kubectl logs web
kubectl logs web --previous  # â† Logs before crash

# Describe Pod
kubectl describe pod web
# Last State:     Terminated
#   Reason:       Error
#   Exit Code:    1

# Check events
kubectl get events --sort-by='.lastTimestamp'
```

**Common causes:**

**A. Missing environment variables:**
```yaml
# Fix: Add required env vars
env:
- name: DATABASE_URL
  value: "postgres://..."
```

**B. Wrong command:**
```yaml
# Fix: Correct command
command: ["python", "app.py"]  # NOT ["python app.py"]
```

**C. Port conflict:**
```yaml
# Fix: Use different port
ports:
- containerPort: 8080  # NOT 80 (may be privileged)
```

### Issue 2: Liveness probe killing healthy container

**Symptoms:**
```bash
kubectl get pods
# NAME   READY   STATUS    RESTARTS   AGE
# web    1/1     Running   50         10m  â† Too many restarts!
```

**Cause:** Probe timing too aggressive

**Fix:**
```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30  # â† Increase (was 5)
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 5  # â† Increase (was 3)
```

### Issue 3: Pods not rescheduling after Node failure

**Symptoms:**
```bash
kubectl get pods
# NAME   READY   STATUS    RESTARTS   AGE   NODE
# web    1/1     Unknown   0          10m   node-1  â† Node down
```

**Cause:** No controller managing Pods (bare Pod, not Deployment)

**Fix:**
```bash
# Delete stuck Pod
kubectl delete pod web --force --grace-period=0

# Create Deployment instead
kubectl create deployment web --image=nginx --replicas=3
```

---

## ğŸ’¡ Best Practices

### 1. Always use controllers (Deployment, StatefulSet)

âŒ **Bad:**
```yaml
apiVersion: v1
kind: Pod  # â† Bare Pod, no self-healing!
metadata:
  name: web
spec:
  containers:
  - name: app
    image: nginx
```

âœ… **Good:**
```yaml
apiVersion: apps/v1
kind: Deployment  # â† Controller ensures replicas!
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: app
        image: nginx
```

### 2. Configure appropriate restartPolicy

```yaml
# Web servers
restartPolicy: Always

# Batch jobs
restartPolicy: OnFailure

# One-time tasks
restartPolicy: Never
```

### 3. Set resource limits (prevent OOMKilled)

```yaml
resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"  # â† Prevent unlimited memory usage
    cpu: "500m"
```

### 4. Implement health checks

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30  # â† Wait for app to start
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```

### 5. Use PodDisruptionBudget

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-pdb
spec:
  minAvailable: 2  # â† Always keep 2 Pods running
  selector:
    matchLabels:
      app: web
```

### 6. Run multiple replicas

```yaml
spec:
  replicas: 3  # â† Not 1! (single point of failure)
```

### 7. Spread Pods across Nodes

```yaml
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: web
```

---

## ğŸ“ Key Takeaways

### Concepts

1. **Self-Healing:** Tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  sá»­a lá»—i mÃ  khÃ´ng cáº§n can thiá»‡p thá»§ cÃ´ng
2. **Control Loop:** LiÃªn tá»¥c so sÃ¡nh desired state vs current state, tá»± Ä‘á»™ng reconcile
3. **Container Crash:** kubelet tá»± Ä‘á»™ng restart (based on restartPolicy)
4. **Health Check Failed:** Liveness probe fail â†’ kubelet restart container
5. **Pod Deleted:** ReplicaSet Controller táº¡o Pod má»›i
6. **Node Failure:** Node Controller evict Pods, reschedule to healthy Nodes
7. **RestartPolicy:**
   - `Always`: LuÃ´n restart (web servers)
   - `OnFailure`: Chá»‰ restart khi fail (batch jobs)
   - `Never`: KhÃ´ng restart (one-time tasks)

### Self-Healing Scenarios

| Scenario | Detection | Action | Recovery Time |
|----------|-----------|--------|---------------|
| Container crash | kubelet | Restart container | ~5s |
| Health check fail | Liveness probe | Restart container | ~30s |
| Pod deleted | ReplicaSet Controller | Create new Pod | ~10s |
| Node failure | Node Controller | Evict + reschedule | ~5min |
| OOMKilled | Linux OOM killer | Restart container | ~5s |
| Disk pressure | kubelet | Evict Pods | ~5min |

### Commands

```bash
# Watch Pod restart
kubectl get pod <name> -w

# Check restart count
kubectl get pod <name> -o jsonpath='{.status.containerStatuses[0].restartCount}'

# View logs before crash
kubectl logs <name> --previous

# Describe Pod (see events)
kubectl describe pod <name>

# Delete Pod (test self-healing)
kubectl delete pod <name>

# Drain Node (simulate failure)
kubectl drain <node> --ignore-daemonsets

# Restore Node
kubectl uncordon <node>
```

---

**ChÃºc má»«ng!** Báº¡n Ä‘Ã£ hiá»ƒu vá» **Self-Healing** trong Kubernetes! ğŸ‰

ğŸ‘‰ [**8.2. Health Checks**](./02-health-checks.md)

---

[â¬…ï¸ 7.3. StorageClass](../07-storage/03-storage-classes.md) | [â¬†ï¸ Pháº§n 8](./README.md) | [â¡ï¸ 8.2. Health Checks](./02-health-checks.md) | [ğŸ  Má»¥c Lá»¥c](../README.md)

