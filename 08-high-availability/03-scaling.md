# 8.3. Scaling - Tá»± Äá»™ng Má»Ÿ Rá»™ng

> Tá»± Ä‘á»™ng má»Ÿ rá»™ng/thu nhá» á»©ng dá»¥ng dá»±a trÃªn nhu cáº§u thá»±c táº¿

---

## ğŸ“– Má»¥c Lá»¥c

1. [Scaling lÃ  gÃ¬?](#-scaling-lÃ -gÃ¬)
2. [Manual Scaling](#-manual-scaling)
3. [Horizontal Pod Autoscaler (HPA)](#-horizontal-pod-autoscaler-hpa)
4. [Vertical Pod Autoscaler (VPA)](#-vertical-pod-autoscaler-vpa)
5. [Cluster Autoscaler](#-cluster-autoscaler)
6. [HPA Metrics Deep Dive](#-hpa-metrics-deep-dive)
7. [HPA Behavior Configuration](#-hpa-behavior-configuration)
8. [Hands-on Labs](#-hands-on-labs)
9. [Troubleshooting](#-troubleshooting)
10. [Best Practices](#-best-practices)

---

## ğŸ¤” Scaling lÃ  gÃ¬?

### Äá»‹nh nghÄ©a

**Scaling** lÃ  kháº£ nÄƒng tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh tÃ i nguyÃªn Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u:
- ğŸ“ˆ **Scale Up/Out:** TÄƒng tÃ i nguyÃªn khi traffic tÄƒng
- ğŸ“‰ **Scale Down/In:** Giáº£m tÃ i nguyÃªn khi traffic giáº£m
- ğŸ’° **Cost optimization:** Chá»‰ tráº£ tiá»n cho tÃ i nguyÃªn thá»±c sá»± cáº§n
- ğŸš€ **Performance:** Äáº£m báº£o response time tá»‘t

### Váº¥n Ä‘á» náº¿u khÃ´ng cÃ³ Auto-scaling

**âŒ Scenario: E-commerce Black Friday**

```
Normal traffic: 1000 req/s â†’ 10 Pods (OK)

Black Friday:
09:00 - Traffic: 50,000 req/s â†’ 10 Pods (overwhelmed!)
        â”œâ”€â”€ CPU: 98% (maxed out)
        â”œâ”€â”€ Response time: 10 seconds (slow!)
        â”œâ”€â”€ Many requests timeout âŒ
        â””â”€â”€ Lost sales! ğŸ’¸

Manual scaling:
09:30 - Admin wakes up, scales to 100 Pods
10:00 - Pods running, performance restored
        â””â”€â”€ But: Lost 1 hour of sales! ğŸ”¥

After Black Friday:
23:00 - Traffic: 1000 req/s (back to normal)
        â”œâ”€â”€ Still running 100 Pods (99% idle!)
        â””â”€â”€ Wasting money on 90 unused Pods! ğŸ’¸
```

**âœ… With Auto-scaling:**

```
Normal traffic: 1000 req/s â†’ 10 Pods (OK)

Black Friday:
09:00 - Traffic: 50,000 req/s
09:01 - HPA detects CPU > 70%
09:02 - HPA scales to 100 Pods (automatic!)
09:05 - All Pods running, performance restored âœ…
        â””â”€â”€ Only 5 minutes of degraded performance!

After Black Friday:
23:00 - Traffic: 1000 req/s (back to normal)
23:01 - HPA detects CPU < 30%
23:06 - HPA scales down to 10 Pods (automatic!)
        â””â”€â”€ No wasted resources! âœ…
```

### 2 Types of Scaling

**Horizontal Scaling (Scale Out/In):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pod 1  â”‚  â†’ Scale out â†’   â”‚  Pod 1  â”‚ â”‚  Pod 2  â”‚ â”‚  Pod 3  â”‚
â”‚ 100% CPUâ”‚                  â”‚ 40% CPU â”‚ â”‚ 40% CPU â”‚ â”‚ 40% CPU â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

More Pods! (horizontal)
```

**Vertical Scaling (Scale Up/Down):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Pod 1    â”‚              â”‚    Pod 1    â”‚
â”‚ cpu: 100m   â”‚  â†’ Scale â†’   â”‚ cpu: 500m   â”‚
â”‚ mem: 128Mi  â”‚     up       â”‚ mem: 512Mi  â”‚
â”‚ (100% used) â”‚              â”‚ (30% used)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Bigger Pod! (vertical)
```

---

## ğŸ–±ï¸ Manual Scaling

### Basic Commands

```bash
# Scale Deployment
kubectl scale deployment web --replicas=10

# Scale ReplicaSet
kubectl scale replicaset web-abc123 --replicas=5

# Scale StatefulSet
kubectl scale statefulset mysql --replicas=3

# Scale using manifest
kubectl scale -f deployment.yaml --replicas=20

# Scale multiple resources
kubectl scale deployment web api db --replicas=5
```

### Verify scaling

```bash
# Watch Pods being created
kubectl get pods -w

# Check Deployment replicas
kubectl get deployment web
# NAME   READY   UP-TO-DATE   AVAILABLE   AGE
# web    10/10   10           10          5m

# Describe Deployment
kubectl describe deployment web
# Replicas:  10 desired | 10 updated | 10 total | 10 available
```

### Use cases

**âœ… Khi nÃ o dÃ¹ng manual scaling:**
- ğŸ—“ï¸ **Scheduled events:** Traffic spike Ä‘Ã£ biáº¿t trÆ°á»›c (product launch, sale)
- ğŸ§ª **Testing:** Load testing, stress testing
- ğŸ“Š **Known patterns:** Traffic patterns hÃ ng ngÃ y/tuáº§n
- ğŸš€ **Quick fix:** Response nhanh trÆ°á»›c khi HPA kÃ­ch hoáº¡t

**Example:**

```bash
# Product launch at 10:00 AM
# Pre-scale 30 minutes before
09:30 $ kubectl scale deployment web --replicas=50

# After launch (traffic stabilizes)
12:00 $ kubectl scale deployment web --replicas=20
```

---

## ğŸ“Š Horizontal Pod Autoscaler (HPA)

### HPA lÃ  gÃ¬?

**HPA** tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh sá»‘ lÆ°á»£ng Pods dá»±a trÃªn metrics (CPU, memory, custom).

### Basic Example

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  minReplicas: 2         # â† Minimum Pods (HA)
  maxReplicas: 10        # â† Maximum Pods (cost control)
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # â† Target: 70% CPU
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

```
Hiá»‡n táº¡i: 2 Pods, CPU: 85% (> 70% target)
    â†“
HPA tÃ­nh toÃ¡n: cáº§n 3 Pods (85% / 70% * 2 â‰ˆ 2.4 â†’ lÃ m trÃ²n lÃªn 3)
    â†“
HPA scale Deployment lÃªn 3 replicas
    â†“
Pod má»›i khá»Ÿi Ä‘á»™ng
    â†“
CPU: 60% (< 70% target) âœ…
```

### Create HPA (Command)

```bash
# Simple HPA (CPU only)
kubectl autoscale deployment web \
  --min=2 \
  --max=10 \
  --cpu-percent=70

# Verify
kubectl get hpa
```

**Output:**
```
NAME      REFERENCE        TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
web-hpa   Deployment/web   45%/70%   2         10        3          5m
                           â†‘ Current/Target
```

### HPA Algorithm

**Formula:**

```
desiredReplicas = ceil[currentReplicas * (currentMetric / targetMetric)]
```

**Example:**

```
Current: 5 Pods
Current CPU: 85%
Target CPU: 70%

desiredReplicas = ceil[5 * (85 / 70)]
                = ceil[5 * 1.214]
                = ceil[6.07]
                = 7 Pods

HPA scales to 7 Pods! âœ…
```

### Timeline Example

```
00:00 - Deployment created, replicas: 2
        â””â”€â”€ CPU: 30% (normal traffic)

00:00 - HPA created (target: 70%)
        â””â”€â”€ No action needed (30% < 70%)

01:00 - Traffic spike!
        â”œâ”€â”€ CPU: 85% (> 70% target)
        â””â”€â”€ Current: 2 Pods

01:00 - HPA calculates
        â””â”€â”€ desiredReplicas = ceil[2 * (85/70)] = 3

01:01 - HPA updates Deployment
        â””â”€â”€ replicas: 2 â†’ 3

01:02 - New Pod created
        â””â”€â”€ Status: Running, Ready: 1/1

01:03 - Traffic distributed to 3 Pods
        â””â”€â”€ CPU: 60% (< 70% target) âœ…

01:30 - More traffic!
        â”œâ”€â”€ CPU: 90% (> 70%)
        â””â”€â”€ desiredReplicas = ceil[3 * (90/70)] = 4

01:31 - HPA scales to 4 Pods
        â””â”€â”€ CPU: 65% âœ…

02:00 - Traffic decreases
        â”œâ”€â”€ CPU: 40% (< 70%)
        â””â”€â”€ desiredReplicas = ceil[4 * (40/70)] = 3

02:05 - HPA waits (stabilization window: 5 min)
        â””â”€â”€ "Maybe temporary drop..."

07:05 - Still low (40%), scale down
        â””â”€â”€ replicas: 4 â†’ 3

07:10 - CPU: 55% âœ…
```

### Prerequisites

**â— REQUIRED: Resource requests must be set!**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: app
        image: my-app:1.0
        resources:
          requests:
            cpu: 200m      # â† REQUIRED for CPU-based HPA!
            memory: 256Mi  # â† REQUIRED for memory-based HPA!
          limits:
            cpu: 500m
            memory: 512Mi
```

**Why?** HPA calculates utilization:

```
CPU Utilization = (Current CPU usage / CPU request) * 100%

Example:
Current usage: 150m
Request: 200m
Utilization = (150 / 200) * 100% = 75%
```

---

## ğŸ“ Vertical Pod Autoscaler (VPA)

### VPA lÃ  gÃ¬?

**VPA** tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh **requests/limits** CPU vÃ  memory cho Pods.

### Khi nÃ o dÃ¹ng VPA vs HPA?

**HPA (Horizontal):**
- âœ… Stateless apps (web servers, APIs)
- âœ… CÃ³ thá»ƒ cháº¡y nhiá»u replicas
- âœ… Scale nhanh

**VPA (Vertical):**
- âœ… Stateful apps (databases, caches)
- âœ… KhÃ´ng thá»ƒ scale horizontal dá»… dÃ ng
- âœ… Cáº§n right-size resources

### VPA Example

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  updatePolicy:
    updateMode: "Auto"  # â† Auto update (recreate Pods)
    # updateMode: "Off"      # Just recommend, don't apply
    # updateMode: "Initial"  # Only set at Pod creation
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2000m
        memory: 2Gi
```

### Äiá»u gÃ¬ xáº£y ra?

**Tráº¡ng thÃ¡i ban Ä‘áº§u:**

```yaml
resources:
  requests:
    cpu: 100m      # â† Initial guess
    memory: 128Mi
```

**Sau 1 tuáº§n monitoring:**

```
VPA quan sÃ¡t:
â”œâ”€â”€ CPU: Thá»±c táº¿ dÃ¹ng 300-400m (liÃªn tá»¥c)
â””â”€â”€ Memory: Thá»±c táº¿ dÃ¹ng 350-450Mi (liÃªn tá»¥c)

VPA Ä‘á» xuáº¥t:
â”œâ”€â”€ cpu: 100m â†’ 500m (tÄƒng lÃªn!)
â””â”€â”€ memory: 128Mi â†’ 512Mi (tÄƒng lÃªn!)
```

**VPA Ã¡p dá»¥ng (updateMode: Auto):**

```
1. Update Pod spec:
   resources:
     requests:
       cpu: 500m      # â† ÄÃ£ cáº­p nháº­t!
       memory: 512Mi  # â† ÄÃ£ cáº­p nháº­t!

2. Recreate Pods vá»›i resources má»›i
3. Pods má»›i cÃ³ right-sized resources! âœ…
```

### VPA Update Modes

**Auto:**
```yaml
updateMode: "Auto"
# âœ… Tá»± Ä‘á»™ng update vÃ  recreate Pods
# âš ï¸  GÃ¢y downtime (Pod restart)
# DÃ¹ng vá»›i: Multiple replicas + PodDisruptionBudget
```

**Off:**
```yaml
updateMode: "Off"
# âœ… Chá»‰ Ä‘á» xuáº¥t, khÃ´ng Ã¡p dá»¥ng
# Use case: Review thá»§ cÃ´ng trÆ°á»›c khi Ã¡p dá»¥ng
```

**Initial:**
```yaml
updateMode: "Initial"
# âœ… Chá»‰ set resources khi táº¡o Pod
# âš ï¸  KhÃ´ng update Pods Ä‘ang cháº¡y
# Use case: Ãt disruption hÆ¡n, nhÆ°ng thÃ­ch nghi cháº­m hÆ¡n
```

### Kiá»ƒm tra VPA recommendations

```bash
# Xem VPA status
kubectl get vpa web-vpa -o yaml

# Kiá»ƒm tra recommendations
kubectl describe vpa web-vpa
```

**Output:**

```yaml
status:
  recommendation:
    containerRecommendations:
    - containerName: app
      lowerBound:
        cpu: 250m
        memory: 256Mi
      target:           # â† VPA applies this!
        cpu: 500m
        memory: 512Mi
      uncappedTarget:
        cpu: 750m       # â† Uncapped recommendation
        memory: 768Mi
      upperBound:
        cpu: 1000m
        memory: 1Gi
```

### âš ï¸ HPA + VPA Conflict!

**âŒ DON'T use HPA and VPA on the same metric:**

```yaml
# HPA scales based on CPU
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      averageUtilization: 70

# VPA adjusts CPU requests
# â†‘ CONFLICT! Both changing CPU!
```

**Problem:**

```
HPA: "CPU is 80%, scale to 5 Pods!"
VPA: "CPU is 80%, increase CPU request to 500m!"
    â†“
Both act simultaneously â†’ unpredictable behavior! ğŸ”¥
```

**âœ… Solution: Use different metrics**

```yaml
# HPA: Scale based on memory
hpa:
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        averageUtilization: 80

# VPA: Adjust CPU only
vpa:
  resourcePolicy:
    containerPolicies:
    - containerName: app
      mode: Auto
      controlledResources: ["cpu"]  # â† Only CPU, not memory!
```

---

## ğŸŒ Cluster Autoscaler

### Cluster Autoscaler lÃ  gÃ¬?

**Cluster Autoscaler** tá»± Ä‘á»™ng thÃªm hoáº·c xÃ³a **Nodes** khá»i cluster.

### Khi nÃ o Ä‘Æ°á»£c trigger?

**Scale Up (ThÃªm Nodes):**
```
HPA scale lÃªn 50 Pods
    â†“
Scheduler cá»‘ gáº¯ng assign Pods
    â†“
âŒ KhÃ´ng Ä‘á»§ CPU/memory trÃªn Nodes hiá»‡n táº¡i!
    â†“
Pods bá»‹ stuck á»Ÿ tráº¡ng thÃ¡i "Pending"
    â†“
Cluster Autoscaler phÃ¡t hiá»‡n
    â†“
ThÃªm Nodes má»›i (qua cloud provider API)
    â†“
Pods Ä‘Æ°á»£c schedule lÃªn Nodes má»›i âœ…
```

**Scale Down (XÃ³a Nodes):**
```
Traffic giáº£m
    â†“
HPA scale xuá»‘ng cÃ²n 10 Pods
    â†“
Má»™t sá»‘ Nodes bá»‹ underutilized (< 50% usage)
    â†“
Cluster Autoscaler phÃ¡t hiá»‡n
    â†“
Evict Pods an toÃ n khá»i Node underutilized
    â†“
XÃ³a Node (qua cloud provider API)
    â†“
Tiáº¿t kiá»‡m tiá»n! ğŸ’°
```

### Timeline Example

```
09:00 - Cluster: 3 Nodes, 30 Pods
        â”œâ”€â”€ Node 1: 10 Pods (80% CPU)
        â”œâ”€â”€ Node 2: 10 Pods (80% CPU)
        â””â”€â”€ Node 3: 10 Pods (80% CPU)

09:00 - Traffic spike!
        â””â”€â”€ HPA scales web Deployment: 10 â†’ 50 replicas

09:01 - 40 new Pods created
        â””â”€â”€ Status: Pending (no Node can fit them!)
        
09:01 - Cluster Autoscaler detects
        â””â”€â”€ "Need more Nodes!"

09:02 - Cluster Autoscaler calls cloud API
        â””â”€â”€ "AWS: Create 2 new m5.xlarge instances"

09:05 - New Nodes join cluster
        â”œâ”€â”€ Node 4: Ready
        â””â”€â”€ Node 5: Ready

09:06 - Scheduler assigns Pending Pods
        â”œâ”€â”€ Node 4: 20 Pods
        â””â”€â”€ Node 5: 20 Pods

09:10 - All 50 Pods running! âœ…
        â””â”€â”€ Cluster: 5 Nodes, 50 Pods

15:00 - Traffic drops
        â””â”€â”€ HPA scales: 50 â†’ 10 replicas

15:05 - Cluster status:
        â”œâ”€â”€ Node 1: 3 Pods (30% CPU)
        â”œâ”€â”€ Node 2: 3 Pods (30% CPU)
        â”œâ”€â”€ Node 3: 3 Pods (30% CPU)
        â”œâ”€â”€ Node 4: 1 Pod  (10% CPU) â† Underutilized!
        â””â”€â”€ Node 5: 0 Pods (0% CPU)  â† Empty!

15:15 - Cluster Autoscaler (after 10min wait)
        â””â”€â”€ "Node 5 is empty, remove it!"

15:16 - Node 5 removed
        â””â”€â”€ Save money! ğŸ’°

15:30 - Cluster Autoscaler
        â””â”€â”€ "Node 4 underutilized, move Pod to Node 1-3"

15:31 - Pod evicted from Node 4, rescheduled to Node 2

15:35 - Node 4 removed
        â””â”€â”€ Cluster: 3 Nodes, 10 Pods (back to normal!)
```

### Cloud Provider Setup

**AWS EKS:**

```yaml
# cluster-autoscaler.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - name: cluster-autoscaler
        image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.27.0
        command:
        - ./cluster-autoscaler
        - --cloud-provider=aws
        - --namespace=kube-system
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my-cluster
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
```

**GCP GKE:**

```bash
# Enable Cluster Autoscaler on node pool
gcloud container clusters update my-cluster \
  --enable-autoscaling \
  --min-nodes=3 \
  --max-nodes=10 \
  --node-pool=default-pool
```

**Azure AKS:**

```bash
# Enable Cluster Autoscaler
az aks update \
  --resource-group myResourceGroup \
  --name myAKSCluster \
  --enable-cluster-autoscaler \
  --min-count 3 \
  --max-count 10
```

---

## ğŸ“ˆ HPA Metrics Deep Dive

### 1. Resource Metrics (CPU, Memory)

**CPU-based:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # â† 70% of CPU request
```

**Memory-based:**

```yaml
metrics:
- type: Resource
  resource:
    name: memory
    target:
      type: Utilization
      averageUtilization: 80  # â† 80% of memory request
```

**Both CPU and Memory:**

```yaml
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      type: Utilization
      averageUtilization: 70
- type: Resource
  resource:
    name: memory
    target:
      type: Utilization
      averageUtilization: 80

# â†‘ HPA scales based on HIGHEST utilization!
# If CPU: 60%, Memory: 85% â†’ Scale based on Memory (85%)
```

### 2. Custom Metrics (Application-specific)

**HTTP requests per second:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"  # â† 1000 req/s per Pod
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

```
Metric: http_requests_total = 1000

ÄÆ°á»£c enriched vá»›i K8s metadata:
  â€¢ pod_name: web-abc123
  â€¢ namespace: production
  â€¢ app: web
  â€¢ version: v1.2.3
  â€¢ tier: frontend
  â€¢ environment: prod
  â€¢ node: node-1
  â€¢ cluster: prod-cluster
  â€¢ team: platform (tá»« annotation)

Query trong Datadog/Dynatrace:
  "Hiá»ƒn thá»‹ http_requests_total vá»›i environment=prod VÃ€ tier=frontend"
```

### 3. External Metrics (Queue depth, etc.)

**RabbitMQ queue depth:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: worker
  minReplicas: 1
  maxReplicas: 50
  metrics:
  - type: External
    external:
      metric:
        name: rabbitmq_queue_depth
        selector:
          matchLabels:
            queue: "tasks"
      target:
        type: AverageValue
        averageValue: "30"  # â† 30 messages per Pod
```

**VÃ­ dá»¥:**

```
Queue: 500 messages
Hiá»‡n táº¡i: 5 worker Pods
Average má»—i Pod: 500 / 5 = 100 messages

Target: 30 messages má»—i Pod
desiredReplicas = ceil[5 * (100 / 30)] = 17 Pods

HPA scale lÃªn 17 Pods! âœ…
Average má»›i: 500 / 17 â‰ˆ 29 messages má»—i Pod âœ…
```

---

## âš™ï¸ HPA Behavior Configuration

### HÃ nh Vi Máº·c Äá»‹nh

**Váº¥n Ä‘á» vá»›i default:**

```
Traffic spike â†’ Scale up ngay láº­p tá»©c
Traffic drop â†’ Scale down ngay láº­p tá»©c
    â†“
Flapping! (scale up/down liÃªn tá»¥c)
    â†“
Pod churn, instability! ğŸ”¥
```

### Stabilization Windows

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0  # â† Scale up immediately (default)
      policies:
      - type: Percent
        value: 100       # â† Can double Pods
        periodSeconds: 15  # â† Every 15s
      - type: Pods
        value: 4         # â† Or add 4 Pods
        periodSeconds: 15
      selectPolicy: Max  # â† Use policy that adds MORE Pods
    
    scaleDown:
      stabilizationWindowSeconds: 300  # â† Wait 5 minutes before scale down!
      policies:
      - type: Percent
        value: 50        # â† Max 50% scale down at once
        periodSeconds: 60  # â† Every 60s
      - type: Pods
        value: 2         # â† Or remove 2 Pods
        periodSeconds: 60
      selectPolicy: Min  # â† Use policy that removes FEWER Pods
```

### Scale Up Example

```
00:00 - Current: 2 Pods, CPU: 85%
        â””â”€â”€ desiredReplicas = ceil[2 * (85/70)] = 3

00:00 - Policy 1 (Percent): 2 * 100% = 2 (can add up to 2)
        Policy 2 (Pods): Can add 4 Pods
        selectPolicy: Max â†’ Add 2 Pods (limited by Percent policy)

00:01 - Current: 4 Pods (2+2)

00:01 - Still high CPU: 80%
        â””â”€â”€ desiredReplicas = ceil[4 * (80/70)] = 5

00:01 - (wait 15s before next scale)

00:16 - Policy 1: 4 * 100% = 4 (can add up to 4)
        Policy 2: Can add 4 Pods
        selectPolicy: Max â†’ Add 4 Pods

00:17 - Current: 8 Pods (4+4)
        â””â”€â”€ CPU: 60% âœ… (below target)
```

### Scale Down Example

```
10:00 - Current: 10 Pods, CPU: 40%
        â””â”€â”€ desiredReplicas = ceil[10 * (40/70)] = 6

10:00 - HPA wants to scale down to 6 Pods
        â””â”€â”€ But: stabilizationWindowSeconds: 300

10:00 - HPA waits... "Maybe temporary drop?"

10:05 - Still low (CPU: 40%)

15:05 - 5 minutes passed, OK to scale down now!

15:05 - Policy 1 (Percent): 10 * 50% = 5 (remove max 5)
        Policy 2 (Pods): Remove 2 Pods
        selectPolicy: Min â†’ Remove 2 Pods (more conservative)

15:06 - Current: 8 Pods (10-2)

15:06 - CPU: 50% (still low)
        â””â”€â”€ desiredReplicas = 6 (want to remove 2 more)

15:06 - (wait 60s before next scale)

16:06 - Remove 2 more Pods

16:07 - Current: 6 Pods
        â””â”€â”€ CPU: 65% âœ… (close to target)
```

---

## ğŸ§ª Hands-on Labs

### Lab 1: Setup HPA with CPU metric

**Step 1: Install Metrics Server**

```bash
# Check if Metrics Server installed
kubectl get deployment metrics-server -n kube-system

# If not, install:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Verify
kubectl top nodes
kubectl top pods
```

**Step 2: Create Deployment with resource requests**

```yaml
# web-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: app
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 200m      # â† REQUIRED!
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
```

```bash
kubectl apply -f web-deployment.yaml
```

**Step 3: Create HPA**

```bash
# Create HPA
kubectl autoscale deployment web \
  --min=2 \
  --max=10 \
  --cpu-percent=50

# Or using YAML:
kubectl apply -f - <<EOF
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
EOF
```

**Step 4: Check HPA status**

```bash
kubectl get hpa web-hpa
```

**Output:**
```
NAME      REFERENCE        TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
web-hpa   Deployment/web   0%/50%    2         10        2          1m
```

**Step 5: Generate load**

```bash
# Get Service URL
kubectl get svc web

# In another terminal, generate load
kubectl run -it --rm load-generator \
  --image=busybox \
  --restart=Never \
  -- /bin/sh -c "while true; do wget -q -O- http://web; done"
```

**Step 6: Watch HPA scale**

```bash
# Watch HPA
kubectl get hpa web-hpa -w

# Watch Pods
kubectl get pods -l app=web -w
```

**Expected:**
```
NAME      REFERENCE        TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
web-hpa   Deployment/web   0%/50%     2         10        2          2m
web-hpa   Deployment/web   75%/50%    2         10        2          3m  â† High CPU!
web-hpa   Deployment/web   75%/50%    2         10        3          3m  â† Scaled to 3!
web-hpa   Deployment/web   55%/50%    2         10        3          4m
web-hpa   Deployment/web   48%/50%    2         10        3          5m  â† Stable
```

**Step 7: Stop load and watch scale down**

```bash
# Stop load generator (Ctrl+C)

# Watch scale down (takes ~5 minutes)
kubectl get hpa web-hpa -w
```

### Lab 2: HPA with custom metrics (HTTP requests)

**YÃªu cáº§u:** ÄÃ£ cÃ i Prometheus + Prometheus Adapter

```bash
# Install Prometheus Adapter
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus-adapter prometheus-community/prometheus-adapter
```

**Application with metrics:**

```yaml
# app-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: my-api-with-metrics:1.0
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
```

**HPA with custom metric:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"  # â† 100 req/s per Pod
```

```bash
kubectl apply -f api-hpa.yaml

# Check
kubectl get hpa api-hpa
```

---

## ğŸ”§ Troubleshooting

### Issue 1: HPA shows "unknown" targets

**Symptoms:**

```bash
kubectl get hpa
# NAME      REFERENCE        TARGETS         MINPODS   MAXPODS   REPLICAS
# web-hpa   Deployment/web   <unknown>/70%   2         10        2
```

**Causes:**

**A. Metrics Server not installed**

```bash
# Check
kubectl get deployment metrics-server -n kube-system
# Error: deployments.apps "metrics-server" not found

# Fix: Install Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

**B. No resource requests defined**

```bash
kubectl get deployment web -o yaml | grep -A 5 resources
# (empty or no requests)

# Fix: Add resource requests
kubectl set resources deployment web \
  --requests=cpu=200m,memory=256Mi \
  --limits=cpu=500m,memory=512Mi
```

**C. Pods not ready**

```bash
kubectl get pods -l app=web
# NAME       READY   STATUS    RESTARTS   AGE
# web-abc    0/1     Running   0          1m  â† Not ready!

# Fix: Check Pod logs, fix readiness probe
```

### Issue 2: HPA not scaling up

**Symptoms:**

```bash
# CPU is 95%, but still 2 Pods!
kubectl get hpa
# NAME      REFERENCE        TARGETS    MINPODS   MAXPODS   REPLICAS
# web-hpa   Deployment/web   95%/70%    2         10        2
```

**Debug:**

```bash
# Check HPA events
kubectl describe hpa web-hpa
```

**Causes:**

**A. maxReplicas reached**

```
Events:
  Warning  TooManyReplicas  HPA reached maximum replicas (10)
```

**Fix:**
```bash
kubectl patch hpa web-hpa -p '{"spec":{"maxReplicas":20}}'
```

**B. Insufficient cluster resources**

```
Events:
  Warning  FailedGetResourceMetric  unable to get metrics for resource cpu
```

**Fix:**
```bash
# Check Node resources
kubectl top nodes

# May need Cluster Autoscaler or add Nodes manually
```

**C. Scale-up cooldown**

```bash
# HPA has default cooldown: 3 minutes between scale-ups
# Wait and check again
```

### Issue 3: HPA flapping (scale up/down repeatedly)

**Symptoms:**

```bash
kubectl get hpa web-hpa -w
# web-hpa   Deployment/web   65%/70%    2    10    3     5m
# web-hpa   Deployment/web   72%/70%    2    10    4     5m  â† Scale up
# web-hpa   Deployment/web   68%/70%    2    10    4     6m
# web-hpa   Deployment/web   69%/70%    2    10    3     7m  â† Scale down
# web-hpa   Deployment/web   73%/70%    2    10    4     8m  â† Scale up again!
```

**Fix: Add stabilization window**

```yaml
behavior:
  scaleDown:
    stabilizationWindowSeconds: 300  # â† Wait 5 minutes
    policies:
    - type: Percent
      value: 50
      periodSeconds: 60
```

```bash
kubectl apply -f web-hpa-with-behavior.yaml
```

---

## ğŸ’¡ Best Practices

### 1. Always set resource requests

âŒ **Bad:**
```yaml
containers:
- name: app
  image: my-app
  # No resource requests! âŒ
```

âœ… **Good:**
```yaml
containers:
- name: app
  image: my-app
  resources:
    requests:
      cpu: 200m      # â† REQUIRED for HPA!
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
```

### 2. Set reasonable min/max replicas

âŒ **Bad:**
```yaml
minReplicas: 1   # â† No HA!
maxReplicas: 1000  # â† Cost explosion!
```

âœ… **Good:**
```yaml
minReplicas: 2   # â† HA (minimum 2)
maxReplicas: 50  # â† Cost control (reasonable max)
```

### 3. Use stabilization windows

```yaml
behavior:
  scaleUp:
    stabilizationWindowSeconds: 0  # Fast scale up (OK for traffic spikes)
  scaleDown:
    stabilizationWindowSeconds: 300  # Slow scale down (avoid flapping)
```

### 4. Don't use HPA + VPA on same metric

âŒ **Bad:**
```yaml
# HPA scales based on CPU
# VPA adjusts CPU requests
# â†‘ CONFLICT!
```

âœ… **Good:**
```yaml
# HPA: Scale based on memory
# VPA: Adjust CPU requests only
# â†‘ No conflict!
```

### 5. Monitor HPA metrics

```bash
# Check HPA status
kubectl get hpa

# Check events
kubectl describe hpa <name>

# Check Pod metrics
kubectl top pods
```

### 6. Use PodDisruptionBudget with VPA

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-pdb
spec:
  minAvailable: 2  # â† Keep 2 Pods available during VPA updates
  selector:
    matchLabels:
      app: web
```

### 7. Test scaling before production

```bash
# Load test
kubectl run -it --rm load-generator \
  --image=busybox \
  --restart=Never \
  -- /bin/sh -c "while true; do wget -q -O- http://web; done"

# Watch scaling
kubectl get hpa -w
kubectl get pods -w
```

---

## ğŸ“ Key Takeaways

### Scaling Types

1. **Manual Scaling:** `kubectl scale deployment web --replicas=10`
2. **HPA (Horizontal):** Auto-scale number of Pods (2 â†’ 10 Pods)
3. **VPA (Vertical):** Auto-adjust CPU/memory requests (100m â†’ 500m)
4. **Cluster Autoscaler:** Auto-add/remove Nodes (3 â†’ 5 Nodes)

### When to Use

| Scaling Type | Use Case | Example |
|--------------|----------|---------|
| **Manual** | Scheduled events, known patterns | Product launch, Black Friday |
| **HPA** | Dynamic traffic (web apps, APIs) | E-commerce, social media |
| **VPA** | Right-size resources (databases) | PostgreSQL, Redis |
| **Cluster Autoscaler** | Dynamic cluster capacity | Cloud environments |

### HPA Requirements

1. âœ… Metrics Server installed
2. âœ… Resource requests defined
3. âœ… Pods ready and healthy
4. âœ… Sufficient cluster capacity (or Cluster Autoscaler)

### HPA Algorithm

```
desiredReplicas = ceil[currentReplicas * (currentMetric / targetMetric)]
```

### Best Practices

- âœ… Set resource requests (required for HPA)
- âœ… Reasonable min (â‰¥2 for HA) and max (cost control)
- âœ… Stabilization windows (avoid flapping)
- âœ… Don't mix HPA + VPA on same metric
- âœ… Use PodDisruptionBudget with VPA
- âœ… Monitor and test before production

### Commands

```bash
# Manual scaling
kubectl scale deployment web --replicas=10

# Create HPA
kubectl autoscale deployment web --min=2 --max=10 --cpu-percent=70

# Check HPA
kubectl get hpa
kubectl describe hpa <name>

# Check metrics
kubectl top nodes
kubectl top pods

# Delete HPA
kubectl delete hpa <name>
```

---

**ChÃºc má»«ng!** HoÃ n thÃ nh **Pháº§n 8: High Availability & Scaling** ğŸ‰

Báº¡n Ä‘Ã£ há»c:
- âœ… Self-Healing (tá»± phá»¥c há»“i)
- âœ… Health Checks (probes)
- âœ… Scaling (manual, HPA, VPA, Cluster Autoscaler)

**Báº¡n Ä‘Ã£ hoÃ n thÃ nh toÃ n bá»™ kiáº¿n thá»©c cá»‘t lÃµi cá»§a Kubernetes!** ğŸ‰ğŸ‰ğŸ‰

ğŸ‘‰ [**Pháº§n 9: Next Steps - Lá»™ TrÃ¬nh Tiáº¿p Theo**](../09-next-steps/README.md)

---

[â¬…ï¸ 8.2. Health Checks](./02-health-checks.md) | [â¬†ï¸ Pháº§n 8](./README.md) | [â¡ï¸ 9. Next Steps](../09-next-steps/README.md) | [ğŸ  Má»¥c Lá»¥c](../README.md)
