# 2.2. Control Plane - Bá»™ NÃ£o Cá»§a Cluster

> Hiá»ƒu sÃ¢u vá» tá»«ng component trong Control Plane

---

## ğŸ¯ Má»¥c TiÃªu

- Hiá»ƒu vai trÃ² chi tiáº¿t cá»§a tá»«ng component
- Biáº¿t cÃ¡ch cÃ¡c components tÆ°Æ¡ng tÃ¡c
- Náº¯m Ä‘Æ°á»£c workflow cá»§a operations
- Troubleshoot Ä‘Æ°á»£c váº¥n Ä‘á» Control Plane

---

## ğŸ›ï¸ Control Plane Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          CONTROL PLANE (Master Node)         â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚          kube-apiserver                â”‚ â”‚ â† Entry point
â”‚  â”‚      (REST API, Authentication)        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚    â”‚           â”‚           â”‚            â”‚   â”‚
â”‚    â–¼           â–¼           â–¼            â–¼   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ etcd â”‚  â”‚Schedulerâ”‚  â”‚Controlâ”‚  â”‚Cloud â”‚â”‚â”‚
â”‚  â”‚      â”‚  â”‚        â”‚  â”‚ ler   â”‚  â”‚Ctrl  â”‚â”‚â”‚
â”‚  â”‚      â”‚  â”‚        â”‚  â”‚Managerâ”‚  â”‚Mgr   â”‚â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  Storage    Placement   Reconcile  Cloud   â”‚â”‚
â”‚                                             â”‚â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ API Server (kube-apiserver)

### ğŸ¯ Vai TrÃ²

**API Server = Gateway duy nháº¥t cá»§a Kubernetes**

- âœ… **Frontend cho Control Plane**
- âœ… **Nháº­n má»i requests** (kubectl, controllers, kubelet...)
- âœ… **Authentication & Authorization**
- âœ… **Validation**
- âœ… **Interface vá»›i etcd**

### ğŸ¢ VÃ­ Dá»¥ Thá»±c Táº¿

**API Server = Receptionist (lá»… tÃ¢n) cá»§a cÃ´ng ty**

```
KhÃ¡ch Ä‘áº¿n (kubectl):
  1. Security check (authentication)
  2. Kiá»ƒm tra quyá»n háº¡n (authorization)
  3. Kiá»ƒm tra yÃªu cáº§u há»£p lá»‡ (validation)
  4. Chuyá»ƒn Ä‘áº¿n bá»™ pháº­n phÃ¹ há»£p (routing)
  5. Ghi nháº­n vÃ o sá»• sÃ¡ch (persist to etcd)
```

### ğŸ”„ Workflow: Create Deployment

```
1. kubectl create deployment
   â†“
2. API Server receives HTTP POST request
   â†“
3. Authentication: "Who are you?"
   - Check certificate, token, or credentials
   â†“
4. Authorization (RBAC): "Can you do this?"
   - Check permissions
   â†“
5. Admission Control: "Is this allowed?"
   - Mutating webhooks (modify request)
   - Validating webhooks (accept/reject)
   â†“
6. Validation: "Is YAML correct?"
   - Schema validation
   - Required fields present
   â†“
7. Persist to etcd
   - Write Deployment object to database
   â†“
8. Return response to kubectl
   "Deployment created"
   â†“
9. Watch mechanism triggers
   - Controller Manager notified
```

### ğŸ”§ Chá»©c NÄƒng Chi Tiáº¿t

#### Authentication (XÃ¡c thá»±c)
**CÃ¢u há»i:** "Báº¡n lÃ  ai?"

**Methods:**
- **Client certificates:** X.509 certs
- **Bearer tokens:** Static tokens, service account tokens
- **Basic auth:** Username/password (deprecated, khÃ´ng nÃªn dÃ¹ng)
- **OpenID Connect:** SSO integration

**Example:**
```bash
# kubectl with certificate
kubectl --client-certificate=user.crt \
        --client-key=user.key \
        --server=https://k8s-api:6443 \
        get pods
```

#### Authorization (PhÃ¢n quyá»n)
**CÃ¢u há»i:** "Báº¡n cÃ³ quyá»n lÃ m viá»‡c nÃ y khÃ´ng?"

**Modes:**
- **RBAC (Role-Based Access Control):** Most common â­
- **ABAC (Attribute-Based):** Complex, less used
- **Webhook:** External authorization service
- **Node:** Special authorization for kubelets

**RBAC Example:**
```yaml
# User "john" can only GET pods in namespace "dev"
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: dev
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: dev
subjects:
- kind: User
  name: john
roleRef:
  kind: Role
  name: pod-reader
```

#### Admission Control
**CÃ¢u há»i:** "Request nÃ y cÃ³ compliant vá»›i policies khÃ´ng?"

**Two phases:**
1. **Mutating Admission:** Modify request
   - Add default values
   - Inject sidecars
   - Set resource limits

2. **Validating Admission:** Accept or reject
   - Enforce naming conventions
   - Require labels
   - Block privileged containers

**Built-in Admission Controllers:**
- `NamespaceLifecycle`: Prevent operations on terminating namespaces
- `LimitRanger`: Enforce resource limits
- `ServiceAccount`: Auto-inject service account tokens
- `ResourceQuota`: Enforce quotas
- `PodSecurityPolicy`: Security policies (deprecated in v1.25)

**Custom Admission Webhooks:**
```yaml
# Example: Require all Deployments have label "team"
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: require-team-label
webhooks:
- name: validate.example.com
  rules:
  - operations: ["CREATE", "UPDATE"]
    apiGroups: ["apps"]
    apiVersions: ["v1"]
    resources: ["deployments"]
  clientConfig:
    service:
      name: validation-service
      namespace: default
```

### ğŸ“¡ API Server Features

#### RESTful API
```bash
# All kubectl commands = HTTP requests

# kubectl get pods
GET /api/v1/namespaces/default/pods

# kubectl create -f deployment.yaml
POST /apis/apps/v1/namespaces/default/deployments

# kubectl delete pod nginx
DELETE /api/v1/namespaces/default/pods/nginx
```

#### Watch Mechanism
**Controllers watch for changes:**
```
Controller:
  watch /api/v1/pods
  
API Server:
  Keeps connection open
  Sends events when changes occur:
    ADDED: new pod created
    MODIFIED: pod updated
    DELETED: pod removed
```

**VÃ­ dá»¥:**
```bash
# Watch pods (like controllers do)
kubectl get pods --watch

# Output:
# NAME    STATUS    AGE
# nginx   Pending   0s     â† ADDED event
# nginx   Running   5s     â† MODIFIED event
# nginx   Terminating 30s  â† MODIFIED event
#                          â† DELETED event
```

### ğŸ”’ Security Best Practices

1. **Enable TLS:** Always use HTTPS
2. **Strong authentication:** Use certificates, not passwords
3. **RBAC enabled:** Principle of least privilege
4. **Audit logging:** Log all API calls
5. **Network policies:** Restrict access to API server
6. **Update regularly:** Patch security vulnerabilities

---

## 2ï¸âƒ£ etcd

### ğŸ¯ Vai TrÃ²

**etcd = Database cá»§a Kubernetes**

- âœ… **Distributed key-value store**
- âœ… **LÆ°u entire cluster state**
- âœ… **Consistent vÃ  highly-available**
- âœ… **Source of truth**

### ğŸ¢ VÃ­ Dá»¥ Thá»±c Táº¿

**etcd = Kho lÆ°u trá»¯ há»“ sÆ¡ cá»§a cÃ´ng ty**

```
Má»i thÃ´ng tin quan trá»ng:
- Danh sÃ¡ch nhÃ¢n viÃªn (Pods)
- CÆ¡ cáº¥u tá»• chá»©c (Deployments, Services)
- Lá»‹ch sá»­ thay Ä‘á»•i (Revisions)
- Cáº¥u hÃ¬nh (ConfigMaps, Secrets)

Äáº·c Ä‘iá»ƒm:
- LuÃ´n cáº­p nháº­t (consistent)
- Sao lÆ°u nhiá»u báº£n (distributed)
- Má»i quyáº¿t Ä‘á»‹nh dá»±a trÃªn dá»¯ liá»‡u nÃ y
```

### ğŸ’¾ etcd Stores

**Everything in Kubernetes:**
- Pods, Deployments, Services
- ConfigMaps, Secrets
- Namespaces
- Resource quotas
- RBAC roles
- Network policies
- Custom resources

**Data structure:**
```
/registry/pods/default/nginx
/registry/deployments/production/web-app
/registry/services/default/api-service
/registry/secrets/default/db-credentials
...
```

### ğŸ” etcd Internals

#### Raft Consensus Algorithm
**Problem:** Nhiá»u etcd nodes, lÃ m sao Ä‘áº£m báº£o consistent?

**Solution:** Raft consensus

```
etcd cluster (3 members):
  Node 1 (Leader)    â† Nháº­n writes
  Node 2 (Follower)  â† Replicate
  Node 3 (Follower)  â† Replicate

Write flow:
  1. Client sends write to Leader
  2. Leader replicates to Followers
  3. Wait for majority (2/3) to confirm
  4. Commit write
  5. Return success
```

**Quorum (Majority):**
- 1 node: No HA (single point of failure)
- 2 nodes: âŒ Can't form quorum if 1 fails
- 3 nodes: âœ… Tolerates 1 failure (recommended minimum)
- 5 nodes: âœ… Tolerates 2 failures (production)
- 7 nodes: âœ… Tolerates 3 failures (large clusters)

**Rule:** `quorum = (n/2) + 1`

#### Watch & Notification
**Efficient change detection:**
```
API Server watches etcd:
  watch /registry/pods/
  
etcd:
  When pod changes â†’ Notify API Server immediately
  No polling needed â†’ Efficient
```

### ğŸ›¡ï¸ etcd Best Practices

1. **Odd number of members:** 3, 5, or 7
2. **Fast disks:** SSD required (etcd is I/O intensive)
3. **Separate etcd cluster:** Don't colocate with heavy workloads
4. **Regular backups:** Disaster recovery
5. **Monitor latency:** Should be < 10ms
6. **Secure:** TLS for client-server and peer-to-peer

### ğŸ’¾ Backup & Restore

**Backup:**
```bash
# Backup etcd snapshot
etcdctl snapshot save /backup/etcd-snapshot.db

# Verify backup
etcdctl snapshot status /backup/etcd-snapshot.db
```

**Restore:**
```bash
# Restore from snapshot
etcdctl snapshot restore /backup/etcd-snapshot.db \
  --data-dir=/var/lib/etcd-restore
```

**Frequency:** Daily or more frequent (production)

---

## 3ï¸âƒ£ Scheduler (kube-scheduler)

### ğŸ¯ Vai TrÃ²

**Scheduler = HR Manager**

- âœ… **Assign Pods to Nodes**
- âœ… **Resource optimization**
- âœ… **Constraint satisfaction**
- âœ… **Load balancing**

### ğŸ¢ VÃ­ Dá»¥ Thá»±c Táº¿

**Scheduler = Quáº£n lÃ½ nhÃ¢n sá»± phÃ¢n cÃ´ng cÃ´ng viá»‡c**

```
CÃ³ 1 project má»›i (Pod):
  YÃªu cáº§u:
    - 2 CPU cores
    - 4 GB RAM
    - GPU available
    - Location: Europe zone
  
Scheduler:
  1. TÃ¬m danh sÃ¡ch nhÃ¢n viÃªn (Nodes) phÃ¹ há»£p
  2. Lá»c: Loáº¡i bá» khÃ´ng Ä‘á»§ Ä‘iá»u kiá»‡n
  3. Cháº¥m Ä‘iá»ƒm: Node nÃ o tá»‘t nháº¥t
  4. PhÃ¢n cÃ´ng: Assign Pod to Node
```

### ğŸ”„ Scheduling Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Watch for unscheduled Pods      â”‚
â”‚     (Pods with spec.nodeName="")    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Filtering Phase                 â”‚
â”‚     Find Nodes that fit             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Scoring Phase                   â”‚
â”‚     Rank Nodes by score             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Binding                         â”‚
â”‚     Assign Pod to best Node         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ” Filtering Phase (Predicates)

**Loáº¡i bá» Nodes khÃ´ng phÃ¹ há»£p:**

| Filter | MÃ´ Táº£ | VÃ­ Dá»¥ |
|--------|-------|-------|
| **NodeResourcesFit** | Äá»§ CPU/RAM? | Pod cáº§n 2GB, Node cÃ²n 1GB â†’ Fail |
| **NodeName** | Pod chá»‰ Ä‘á»‹nh Node cá»¥ thá»ƒ? | nodeName: node-1 â†’ Chá»‰ node-1 |
| **NodeSelector** | Match labels? | nodeSelector: gpu=true â†’ Chá»‰ Node cÃ³ GPU |
| **NodeAffinity** | Advanced node selection | Prefer zone=us-west |
| **PodAffinity** | Co-locate vá»›i Pod khÃ¡c? | Web pod gáº§n Redis pod |
| **PodAntiAffinity** | TÃ¡ch xa Pod khÃ¡c? | 2 replicas khÃ¡c Node |
| **Taints/Tolerations** | Node taint? | Node tainted "dedicated=gpu" |
| **Volume** | Volume cÃ³ mount Ä‘Æ°á»£c? | AWS EBS chá»‰ trong 1 AZ |

**Example:**
```
Pod requires:
  CPU: 2 cores
  Memory: 4 GB
  GPU: yes
  Zone: us-west-1a

Cluster:
  Node 1: 8 cores, 16GB, GPU, us-west-1a âœ…
  Node 2: 4 cores, 8GB, no GPU, us-west-1a âŒ (no GPU)
  Node 3: 8 cores, 16GB, GPU, us-east-1a âŒ (wrong zone)

Filtered nodes: [Node 1]
```

### ğŸ“Š Scoring Phase (Priorities)

**Cháº¥m Ä‘iá»ƒm Nodes:**

| Plugin | MÃ´ Táº£ | Score |
|--------|-------|-------|
| **LeastResourceAllocation** | Prefer Node Ã­t workload | More available resources = Higher score |
| **MostResourceAllocation** | Pack Pods densely | Less available resources = Higher score |
| **BalancedResourceAllocation** | Balance CPU/Memory usage | Even CPU and Memory % = Higher score |
| **ImageLocality** | Image Ä‘Ã£ cÃ³ sáºµn? | Image already pulled = Higher score |
| **InterPodAffinity** | Gáº§n Pod affinity? | Match affinity = Higher score |

**Example:**
```
Pod requires: 2 CPU, 4GB RAM

Node 1: 4/8 CPU used, 8/16GB used
  Score: 50 (balanced)

Node 2: 2/8 CPU used, 12/16GB used
  Score: 30 (unbalanced - high memory %)

Node 3: 1/8 CPU used, 2/16GB used
  Score: 80 (lots of free resources)

Winner: Node 3 (highest score)
```

### ğŸ¨ Advanced Scheduling

#### Node Affinity
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - node-1
            - node-2
```

#### Pod Affinity (Co-location)
```yaml
# Place this Pod on same Node as Pods with label app=cache
podAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchExpressions:
      - key: app
        operator: In
        values:
        - cache
    topologyKey: kubernetes.io/hostname
```

#### Taints & Tolerations
```bash
# Taint Node (mark as special-purpose)
kubectl taint nodes node-1 dedicated=gpu:NoSchedule

# Pod must tolerate to schedule on node-1
tolerations:
- key: "dedicated"
  operator: "Equal"
  value: "gpu"
  effect: "NoSchedule"
```

---

## 4ï¸âƒ£ Controller Manager (kube-controller-manager)

### ğŸ¯ Vai TrÃ²

**Controller Manager = GiÃ¡m sÃ¡t viÃªn**

- âœ… **Runs multiple controllers**
- âœ… **Reconcile desired vs actual state**
- âœ… **Self-healing**
- âœ… **Automation**

### ğŸ¢ VÃ­ Dá»¥ Thá»±c Táº¿

**Controller Manager = Tá»• trÆ°á»Ÿng cÃ¡c giÃ¡m sÃ¡t viÃªn**

```
Má»—i Controller = 1 GiÃ¡m sÃ¡t viÃªn chuyÃªn trÃ¡ch:

- Replication Controller:
    "Äáº£m báº£o luÃ´n cÃ³ Ä‘á»§ 3 nhÃ¢n viÃªn ca sÃ¡ng"
    Náº¿u 1 ngÆ°á»i á»‘m â†’ Gá»i ngÆ°á»i thay tháº¿
  
- Node Controller:
    "Kiá»ƒm tra cÃ¡c chi nhÃ¡nh cÃ²n hoáº¡t Ä‘á»™ng khÃ´ng"
    Chi nhÃ¡nh máº¥t liÃªn láº¡c â†’ BÃ¡o cÃ¡o vÃ  xá»­ lÃ½
  
- Endpoints Controller:
    "Cáº­p nháº­t danh báº¡ Ä‘iá»‡n thoáº¡i khi cÃ³ thay Ä‘á»•i"
    NhÃ¢n viÃªn má»›i vÃ o â†’ ThÃªm vÃ o danh báº¡
```

### ğŸ”„ Control Loop Pattern

**Every controller runs this loop:**

```go
for {
  desired_state = get_from_etcd()
  actual_state = observe_reality()
  
  if desired_state != actual_state {
    take_action_to_reconcile()
  }
  
  sleep(resync_period)
}
```

### ğŸ“‹ Built-in Controllers

#### 1. Replication Controller / ReplicaSet Controller
**Äáº£m báº£o sá»‘ lÆ°á»£ng Pod:**
```
Desired: 3 replicas
Actual: 2 replicas (1 Pod crashed)

Action: Create 1 new Pod
```

#### 2. Deployment Controller
**Quáº£n lÃ½ rollouts:**
```
User: kubectl set image deployment/web nginx=1.21

Controller:
  1. Create new ReplicaSet vá»›i image má»›i
  2. Scale up new ReplicaSet (1 â†’ 2 â†’ 3)
  3. Scale down old ReplicaSet (3 â†’ 2 â†’ 1 â†’ 0)
```

#### 3. StatefulSet Controller
**Ordered, stable Pod identities:**
```
Desired: StatefulSet vá»›i 3 Pods

Controller:
  1. Create mysql-0 â†’ Wait until Running
  2. Create mysql-1 â†’ Wait until Running
  3. Create mysql-2 â†’ Wait until Running
  
Scale down:
  1. Delete mysql-2
  2. Wait until terminated
  3. Delete mysql-1
  4. ...
```

#### 4. DaemonSet Controller
**1 Pod per Node:**
```
New Node added to cluster

Controller:
  Detect new Node
  â†’ Create monitoring-agent Pod on that Node
```

#### 5. Job Controller
**Run-to-completion:**
```
Job: Run batch task

Controller:
  1. Create Pod
  2. Monitor Pod
  3. Pod exits with code 0 (success)
  4. Mark Job as Complete
  5. Don't restart Pod
```

#### 6. Node Controller
**Monitor Node health:**
```
Controller checks Node heartbeat:
  - Every 5s: kubelet sends heartbeat
  - No heartbeat for 40s: Mark Node "Unknown"
  - Unknown for 5 minutes: Evict Pods from Node
```

#### 7. Service Controller / Endpoints Controller
**Update Service endpoints:**
```
Service "web" selects Pods with label app=web

Controller watches:
  - Pod with app=web created â†’ Add to endpoints
  - Pod deleted â†’ Remove from endpoints
  - Pod not ready â†’ Remove from endpoints
```

#### 8. Namespace Controller
**Cleanup on namespace deletion:**
```
kubectl delete namespace dev

Controller:
  1. Set namespace status = Terminating
  2. Delete all objects in namespace (Pods, Services, etc)
  3. Wait until all objects deleted
  4. Delete namespace itself
```

### ğŸ¯ Custom Controllers (Operators)

**Extend K8s with custom logic:**

**Example: MySQL Operator**
```yaml
# Custom Resource
apiVersion: mysql.example.com/v1
kind: MySQLCluster
metadata:
  name: my-db
spec:
  replicas: 3
  version: "8.0"
  
# Custom Controller watches MySQLCluster resources
# Creates: StatefulSet, Service, ConfigMap, PV/PVC
# Manages: Backups, failover, upgrades
```

---

## 5ï¸âƒ£ Cloud Controller Manager

### ğŸ¯ Vai TrÃ²

**Integrate vá»›i cloud providers**

- âœ… **Node management** (cloud VMs)
- âœ… **LoadBalancer Service** (create cloud LB)
- âœ… **Route management** (VPC routes)
- âœ… **Volume management** (cloud disks)

### ğŸ¢ VÃ­ Dá»¥

**AWS:**
```
Service type: LoadBalancer

Cloud Controller Manager:
  1. Call AWS API
  2. Create ELB (Elastic Load Balancer)
  3. Configure target group
  4. Update Service with LB hostname
```

**Controllers:**
- **Node Controller:** Sync cloud VMs vá»›i K8s Nodes
- **Route Controller:** Setup network routes
- **Service Controller:** Create/delete cloud load balancers

---

## ğŸ”— Component Interactions

### Scenario: Create Deployment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ kubectl â”‚ kubectl create deployment web --image=nginx
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Server â”‚ 1. Receive request
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ 2. Authenticate, authorize
     â”‚         3. Validate
     â”‚         4. Write to etcd
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  etcd  â”‚ Store Deployment object
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ (watch)
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Deployment       â”‚ 5. Detect new Deployment
â”‚ Controller       â”‚ 6. Create ReplicaSet
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Server â”‚ 7. Write ReplicaSet to etcd
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ (watch)
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ReplicaSet       â”‚ 8. Detect new ReplicaSet
â”‚ Controller       â”‚ 9. Create 3 Pods
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Server â”‚ 10. Write Pods to etcd (unscheduled)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ (watch)
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scheduler        â”‚ 11. Detect unscheduled Pods
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 12. Find best Nodes
       â”‚             13. Bind Pods to Nodes
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Server â”‚ 14. Update Pods with nodeName
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ (kubelet watches on assigned Node)
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ kubelet (Node)   â”‚ 15. Detect Pod assigned to this Node
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 16. Pull image, start container
```

---

## ğŸ“ Key Takeaways

1. **API Server:** Gateway, authentication, validation
2. **etcd:** Database, source of truth, distributed
3. **Scheduler:** Assign Pods to Nodes intelligently
4. **Controller Manager:** Reconcile state, self-healing
5. **Cloud Controller:** Cloud integration
6. **Everything is async:** Components watch and react
7. **Declarative:** State stored, controllers ensure it matches reality

---

## â“ CÃ¢u Há»i Tá»± Kiá»ƒm Tra

1. API Server lÃ m gÃ¬ vá»›i má»—i request?
2. etcd lÆ°u trá»¯ thÃ´ng tin gÃ¬? Táº¡i sao cáº§n backup?
3. Scheduler quyáº¿t Ä‘á»‹nh Pod cháº¡y trÃªn Node nÃ o báº±ng cÃ¡ch nÃ o?
4. Controller Manager cÃ³ nhiá»‡m vá»¥ gÃ¬?
5. Váº½ flow khi user cháº¡y `kubectl create deployment`

---

## ğŸš€ Tiáº¿p Theo

ğŸ‘‰ [2.3. Worker Node - NÆ¡i Cháº¡y Workload](./03-worker-nodes.md)

---

[â¬…ï¸ 2.1. Overview](./01-overview.md) | [â¬†ï¸ Pháº§n 2: Architecture](./README.md) | [ğŸ  Má»¥c Lá»¥c ChÃ­nh](../README.md)

