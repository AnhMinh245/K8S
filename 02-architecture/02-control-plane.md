# 2.2. Control Plane - Bá»™ NÃ£o Cá»§a Cluster

> Deep dive vÃ o tá»«ng component cá»§a Control Plane

---

## ğŸ¯ Má»¥c TiÃªu Há»c

Sau khi há»c xong pháº§n nÃ y, báº¡n sáº½:
- âœ… Hiá»ƒu **chi tiáº¿t tá»«ng component** cá»§a Control Plane
- âœ… Biáº¿t **vai trÃ² cá»¥ thá»ƒ** cá»§a má»—i component
- âœ… Hiá»ƒu **cÃ¡ch cÃ¡c components tÆ°Æ¡ng tÃ¡c** vá»›i nhau
- âœ… Troubleshoot Ä‘Æ°á»£c **issues liÃªn quan Control Plane**

---

## ğŸ§  Control Plane Components

### Tá»•ng Quan

```
CONTROL PLANE = Bá»˜ NÃƒO Cá»¦A CLUSTER

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CONTROL PLANE NODE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  1. kube-apiserver                       â”‚ â”‚
â”‚  â”‚     â€¢ Cá»•ng vÃ o duy nháº¥t                  â”‚ â”‚
â”‚  â”‚     â€¢ REST API                           â”‚ â”‚
â”‚  â”‚     â€¢ Authentication & Authorization     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  2. etcd                                 â”‚ â”‚
â”‚  â”‚     â€¢ Distributed key-value database     â”‚ â”‚
â”‚  â”‚     â€¢ Store all cluster data             â”‚ â”‚
â”‚  â”‚     â€¢ Single source of truth             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  3. kube-scheduler                       â”‚ â”‚
â”‚  â”‚     â€¢ Watch for unassigned Pods          â”‚ â”‚
â”‚  â”‚     â€¢ Select best Node for Pod           â”‚ â”‚
â”‚  â”‚     â€¢ Consider resources, constraints    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  4. kube-controller-manager              â”‚ â”‚
â”‚  â”‚     â€¢ Run multiple controllers           â”‚ â”‚
â”‚  â”‚     â€¢ Watch & reconcile state            â”‚ â”‚
â”‚  â”‚     â€¢ Ensure desired = actual            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  5. cloud-controller-manager (optional)  â”‚ â”‚
â”‚  â”‚     â€¢ Cloud-specific control logic       â”‚ â”‚
â”‚  â”‚     â€¢ Manage cloud resources             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ API Server (kube-apiserver)

### Vai TrÃ²: Trung TÃ¢m Giao Tiáº¿p

**API Server = Receptionist/Switchboard Operator cá»§a cluster**

```
USER/SYSTEM
    â†“
 API Server  â† Äiá»ƒm vÃ o DUY NHáº¤T
    â†“
All other components
```

### Chá»©c NÄƒng ChÃ­nh

**1. Frontend cho Cluster**
```
Má»i interaction vá»›i cluster Ä‘á»u qua API Server:
â”œâ”€â”€ kubectl commands
â”œâ”€â”€ Dashboard
â”œâ”€â”€ CI/CD tools
â”œâ”€â”€ Custom controllers
â””â”€â”€ Other K8s components

â†’ KHÃ”NG AI cÃ³ thá»ƒ bypass API Server!
```

**2. Authentication & Authorization**
```
Má»—i request pháº£i:
1. Authentication: "Báº¡n lÃ  ai?"
   â”œâ”€â”€ Client certificates
   â”œâ”€â”€ Bearer tokens
   â”œâ”€â”€ Service account tokens
   â””â”€â”€ OpenID Connect (OIDC)

2. Authorization: "Báº¡n Ä‘Æ°á»£c lÃ m gÃ¬?"
   â”œâ”€â”€ RBAC (Role-Based Access Control)
   â”œâ”€â”€ ABAC (Attribute-Based)
   â””â”€â”€ Webhook mode

3. Admission Control: "Request cÃ³ há»£p lá»‡ khÃ´ng?"
   â”œâ”€â”€ Validate resources
   â”œâ”€â”€ Mutate resources (add defaults)
   â””â”€â”€ Custom admission webhooks
```

**3. Validation**
```
API Server validates:
â”œâ”€â”€ Syntax correctness (YAML structure)
â”œâ”€â”€ Required fields present
â”œâ”€â”€ Field values valid (e.g., CPU format: "100m")
â”œâ”€â”€ References exist (e.g., ConfigMap exists)
â””â”€â”€ Quota limits not exceeded
```

**4. etcd Gateway**
```
API Server lÃ  component DUY NHáº¤T giao tiáº¿p vá»›i etcd:

Write path:
User â†’ API Server â†’ Validate â†’ etcd

Read path:
User â†’ API Server â†’ etcd â†’ API Server â†’ User

Other components:
Scheduler/Controllers â†’ API Server â†’ etcd
(KHÃ”NG truy cáº­p etcd trá»±c tiáº¿p!)
```

### VÃ­ Dá»¥: Create Pod Request

```yaml
# user.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:latest
```

**Flow chi tiáº¿t:**

```
1. User runs:
   $ kubectl create -f user.yaml
                â†“
2. kubectl sends HTTP POST to API Server:
   POST /api/v1/namespaces/default/pods
   Body: Pod spec (JSON)
                â†“
3. API Server - Authentication:
   âœ“ Verify client certificate
   âœ“ Identity: user@example.com
                â†“
4. API Server - Authorization (RBAC):
   Question: "Can user@example.com create Pods in default namespace?"
   Check RBAC rules...
   âœ“ Allowed!
                â†“
5. API Server - Admission Control:
   â€¢ Mutating admission: Add default values
     - Add imagePullPolicy: Always
     - Add restartPolicy: Always
   â€¢ Validating admission: Check constraints
     - Resource limits OK?
     - Security policies OK?
   âœ“ Valid!
                â†“
6. API Server - Validation:
   âœ“ YAML syntax correct
   âœ“ Required fields present
   âœ“ Image name valid
   âœ“ Container name unique
                â†“
7. API Server writes to etcd:
   Key: /registry/pods/default/nginx
   Value: Pod spec + metadata
   Status: Pending (chÆ°a cÃ³ Node)
                â†“
8. API Server returns to kubectl:
   HTTP 201 Created
   Body: Pod object vá»›i metadata (UID, creationTimestamp)
                â†“
9. kubectl shows:
   pod/nginx created
```

### Commands Ä‘á»ƒ Monitor

```bash
# Check API Server logs
kubectl logs -n kube-system kube-apiserver-<node>

# API Server metrics
kubectl get --raw /metrics | grep apiserver

# Check API Server endpoints
kubectl get --raw /api/v1 | jq
kubectl get --raw /apis | jq

# API Server health
curl -k https://localhost:6443/healthz
curl -k https://localhost:6443/readyz
```

---

## 2ï¸âƒ£ etcd

### Vai TrÃ²: Database Cá»§a Cluster

**etcd = Kho lÆ°u trá»¯ há»“ sÆ¡ cá»§a cÃ´ng ty**

```
etcd lÆ°u Táº¤T Cáº¢:
â”œâ”€â”€ Pods
â”œâ”€â”€ Services
â”œâ”€â”€ Deployments
â”œâ”€â”€ Secrets
â”œâ”€â”€ ConfigMaps
â”œâ”€â”€ Nodes
â””â”€â”€ Má»i K8s resources!
```

### Äáº·c Äiá»ƒm

**1. Distributed Key-Value Store**
```
Key-Value Database:
Key: /registry/pods/default/nginx
Value: {
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": { ... },
  "spec": { ... },
  "status": { ... }
}

Distributed:
â”œâ”€â”€ Run trÃªn multiple nodes (usually 3 or 5)
â”œâ”€â”€ Use Raft consensus algorithm
â”œâ”€â”€ Survive node failures
â””â”€â”€ Always consistent
```

**2. Consistency is Key**
```
Strong Consistency Guarantee:
â”œâ”€â”€ Má»™t write Ä‘Æ°á»£c ack â†’ Guaranteed durable
â”œâ”€â”€ Reads always return latest write
â”œâ”€â”€ No stale data
â””â”€â”€ Perfect for cluster state!

VÃ¬ sao quan trá»ng:
- Cluster state pháº£i accurate 100%
- Pod Ä‘ang cháº¡y á»Ÿ Ä‘Ã¢u?
- Service cÃ³ nhá»¯ng Endpoints nÃ o?
- Wrong data = disaster!
```

**3. Watch Mechanism**
```
Components "watch" API Server for changes:
(NOT etcd directly! Only API Server talks to etcd)

Example - Scheduler watches API Server:
API Server: "New Pod created (status: Pending)"
     â†“
Scheduler: "Aha! Pod cáº§n Node. Let me assign..."
          â†“
Scheduler â†’ API Server: "Assign Pod to Node X"
          â†“
API Server â†’ etcd: Save binding
          â†“
API Server â†’ kubelet (watch): "Pod assigned to you!"
          â†“
kubelet on Node X: "Aha! Let me start this Pod!"

Key Point: Components NEVER access etcd directly!
All communication goes through API Server.
```

### Data Structure trong etcd

```bash
# View etcd structure (requires etcd access)
export ETCDCTL_API=3

# List all keys
etcdctl get / --prefix --keys-only

# Sample structure:
/registry/
â”œâ”€â”€ pods/
â”‚   â”œâ”€â”€ default/
â”‚   â”‚   â”œâ”€â”€ nginx
â”‚   â”‚   â””â”€â”€ webapp-abc123
â”‚   â””â”€â”€ kube-system/
â”‚       â””â”€â”€ coredns-xyz789
â”œâ”€â”€ services/
â”‚   â””â”€â”€ default/
â”‚       â””â”€â”€ kubernetes
â”œâ”€â”€ deployments/
â”‚   â””â”€â”€ default/
â”‚       â””â”€â”€ webapp
â”œâ”€â”€ secrets/
â”‚   â””â”€â”€ default/
â”‚       â””â”€â”€ my-secret
â””â”€â”€ ...

# Get specific Pod
etcdctl get /registry/pods/default/nginx
```

### Backup etcd - Cá»°C Ká»² QUAN TRá»ŒNG!

**Táº I SAO:** etcd cháº¿t = máº¥t TOÃ€N Bá»˜ cluster state!

```bash
# Backup etcd
ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# Verify backup
ETCDCTL_API=3 etcdctl snapshot status /backup/etcd-snapshot.db

# Restore tá»« backup
ETCDCTL_API=3 etcdctl snapshot restore /backup/etcd-snapshot.db \
  --data-dir=/var/lib/etcd-restore

# Schedule regular backups (production must-have!)
# Cron job every 6 hours
```

---

## 3ï¸âƒ£ Scheduler (kube-scheduler)

### Vai TrÃ²: PhÃ¢n CÃ´ng ThÃ´ng Minh

**Scheduler = HR Manager phÃ¢n cÃ´ng nhÃ¢n viÃªn vÃ o projects**

```
Scheduler's job:
1. Watch for Pods without Node assignment (status: Pending)
2. Find best Node for each Pod
3. Assign Pod â†’ Node (write to API Server)
```

### Scheduling Algorithm

**Step 1: Filtering (Lá»c)**
```
Question: "Nodes nÃ o CÃ“ THá»‚ cháº¡y Pod nÃ y?"

Check:
â”œâ”€â”€ Node cÃ³ Ä‘á»§ CPU/RAM khÃ´ng?
â”‚   Pod requests: 1 CPU, 2Gi RAM
â”‚   Node available: 0.5 CPU, 1Gi RAM
â”‚   â†’ FILTERED OUT âŒ
â”‚
â”œâ”€â”€ Node cÃ³ label matching khÃ´ng? (nodeSelector)
â”‚   Pod: nodeSelector: disktype=ssd
â”‚   Node: disktype=hdd
â”‚   â†’ FILTERED OUT âŒ
â”‚
â”œâ”€â”€ Node cÃ³ taints Pod khÃ´ng tolerate?
â”‚   Node: taint=dedicated:NoSchedule
â”‚   Pod: No tolerations
â”‚   â†’ FILTERED OUT âŒ
â”‚
â””â”€â”€ Pod affinity/anti-affinity satisfied?
    â†’ Check rules...

Result: List of feasible nodes
```

**Step 2: Scoring (Cháº¥m Ä‘iá»ƒm)**
```
Question: "Node nÃ o Tá»T NHáº¤T trong cÃ¡c nodes feasible?"

Scoring criteria:
â”œâ”€â”€ LeastRequestedPriority (nhiá»u resources available = Ä‘iá»ƒm cao)
â”‚   Node 1: 20% CPU used â†’ Score: 80
â”‚   Node 2: 60% CPU used â†’ Score: 40
â”‚
â”œâ”€â”€ BalancedResourceAllocation (balanced CPU & RAM = tá»‘t)
â”‚   Node 1: CPU 30%, RAM 80% â†’ Unbalanced â†’ Score: 50
â”‚   Node 2: CPU 60%, RAM 65% â†’ Balanced â†’ Score: 90
â”‚
â”œâ”€â”€ NodeAffinityPriority (match affinity preferences)
â”‚   Preferred affinity matched â†’ +10 points
â”‚
â””â”€â”€ ImageLocalityPriority (image already on node = faster)
    Image present â†’ +5 points

Total scores:
Node 1: 145 points
Node 2: 195 points â† WINNER!
Node 3: 120 points
```

**Step 3: Binding**
```
Scheduler â†’ API Server:
"Assign Pod X to Node 2"

API Server â†’ etcd:
Update Pod:
  spec.nodeName: node-2
  status: Pending (still Pending!)
  
Note: Pod remains "Pending" until kubelet starts it.
Only when container running â†’ status: Running
```

### VÃ­ Dá»¥ Thá»±c Táº¿

**Scenario: Schedule 3 Pods**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx
        resources:
          requests:
            cpu: 500m
            memory: 256Mi
```

**Cluster state:**
```
Nodes:
â”œâ”€â”€ node-1: 2 CPU, 4Gi RAM (available: 1.5 CPU, 3Gi)
â”œâ”€â”€ node-2: 4 CPU, 8Gi RAM (available: 3 CPU, 7Gi)
â””â”€â”€ node-3: 2 CPU, 4Gi RAM (available: 0.3 CPU, 1Gi)
```

**Scheduler decisions:**
```
Pod 1:
â”œâ”€â”€ Filter: node-1 âœ“, node-2 âœ“, node-3 âœ— (not enough CPU)
â”œâ”€â”€ Score: node-2 (195) > node-1 (145)
â””â”€â”€ Assign: Pod 1 â†’ node-2
   (node-2 available: 2.5 CPU, 6.75Gi RAM)

Pod 2:
â”œâ”€â”€ Filter: node-1 âœ“, node-2 âœ“, node-3 âœ—
â”œâ”€â”€ Score: node-2 (175) > node-1 (145)
â””â”€â”€ Assign: Pod 2 â†’ node-2
   (node-2 available: 2 CPU, 6.5Gi RAM)

Pod 3:
â”œâ”€â”€ Filter: node-1 âœ“, node-2 âœ“, node-3 âœ—
â”œâ”€â”€ Score: node-2 (155) > node-1 (145)
â””â”€â”€ Assign: Pod 3 â†’ node-2
   (node-2 available: 1.5 CPU, 6.25Gi RAM)

Final distribution:
â”œâ”€â”€ node-1: 0 Pods (still available)
â”œâ”€â”€ node-2: 3 Pods (all Pods fit!)
â””â”€â”€ node-3: 0 Pods (insufficient resources)

Note: Scheduler prefers node-2 vÃ¬ cÃ³ nhiá»u resources nháº¥t
```

---

## 4ï¸âƒ£ Controller Manager (kube-controller-manager)

### Vai TrÃ²: Äáº£m Báº£o Desired State

**Controller Manager = Operations Manager giÃ¡m sÃ¡t má»i thá»©**

```
Controller Pattern:
1. Watch actual state
2. Compare vá»›i desired state
3. If different â†’ Take action
4. Repeat (forever!)

â†’ Self-healing mechanism!
```

### Controllers ChÃ­nh

**1. Node Controller**
```
Nhiá»‡m vá»¥:
â”œâ”€â”€ Monitor Node health
â”œâ”€â”€ Mark unhealthy Nodes as NotReady
â”œâ”€â”€ Evict Pods tá»« dead Nodes (sau 40s default)
â””â”€â”€ Update Node conditions

Watch loop (every 5s):
for each Node:
  if no heartbeat for 40s:
    Mark Node as NotReady
    Wait 5 minutes
    if still NotReady:
      Delete Pods on that Node
```

**2. Replication Controller / ReplicaSet Controller**
```
Nhiá»‡m vá»¥: Äáº£m báº£o sá»‘ lÆ°á»£ng Pods

Watch loop (every 30s):
Desired replicas: 3
Actual replicas: 2  â† Má»™t Pod crashed!

Action:
â””â”€> Create 1 new Pod

Example:
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-rs
spec:
  replicas: 3  â† Controller maintains this!
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx
```

**3. Endpoints Controller**
```
Nhiá»‡m vá»¥: Update Service Endpoints

Watch:
â”œâ”€â”€ Services created/updated
â””â”€â”€ Pods created/deleted/changed

Action:
Service "webapp" selects app=webapp Pods
  â†’ Find all matching Pods
  â†’ Get their IPs
  â†’ Update Endpoints object

Endpoints object:
apiVersion: v1
kind: Endpoints
metadata:
  name: webapp
subsets:
- addresses:
  - ip: 10.244.1.5  # Pod 1
  - ip: 10.244.2.8  # Pod 2
  - ip: 10.244.1.9  # Pod 3
  ports:
  - port: 80
```

**4. Service Account Controller**
```
Nhiá»‡m vá»¥: Manage ServiceAccounts

Watch:
â””â”€â”€ Namespaces created

Action:
When new Namespace created:
  â†’ Create "default" ServiceAccount
  â†’ Create token Secret for ServiceAccount

Every Pod gets a ServiceAccount (default if not specified)
```

**5. Deployment Controller**
```
Nhiá»‡m vá»¥: Manage Deployments (ReplicaSets + Rolling Updates)

Watch:
â””â”€â”€ Deployment changes

Actions:
â”œâ”€â”€ New Deployment â†’ Create ReplicaSet
â”œâ”€â”€ Update image â†’ Create new ReplicaSet, scale up/down
â”œâ”€â”€ Rollback â†’ Switch to old ReplicaSet
â””â”€â”€ Pause/Resume â†’ Control rollout

Example rolling update:
Deployment: nginx:1.19 â†’ nginx:1.20
1. Create new ReplicaSet (nginx:1.20) with 0 replicas
2. Scale up new (1), scale down old (2)
3. Scale up new (2), scale down old (1)
4. Scale up new (3), scale down old (0)
â†’ Done! Zero downtime!
```

**6. Job Controller**
```
Nhiá»‡m vá»¥: Run Jobs to completion

Watch:
â””â”€â”€ Jobs

Actions:
â”œâ”€â”€ Create Pods for Job
â”œâ”€â”€ Monitor completion
â”œâ”€â”€ Restart Pods náº¿u fail (based on restartPolicy)
â””â”€â”€ Mark Job as Complete/Failed

Example:
apiVersion: batch/v1
kind: Job
metadata:
  name: backup-job
spec:
  completions: 1  # Run once
  template:
    spec:
      containers:
      - name: backup
        image: backup:latest
      restartPolicy: Never
```

### Reconciliation Loop Pattern

**Core pattern cá»§a táº¥t cáº£ controllers:**

```go
// Pseudo-code
for {
  // 1. Watch for changes
  actualState = getActualState()
  desiredState = getDesiredState()
  
  // 2. Compare
  if actualState != desiredState {
    // 3. Take action
    reconcile(actualState, desiredState)
  }
  
  // 4. Wait vÃ  repeat
  sleep(30 * time.Second)
}
```

**VÃ­ dá»¥ ReplicaSet Controller:**

```
Loop iteration 1:
Desired: 3 Pods
Actual: 3 Pods
â†’ No action (all good!)

--- Pod 2 crashes ---

Loop iteration 2:
Desired: 3 Pods
Actual: 2 Pods (Pod 1, Pod 3)
â†’ Action: Create Pod 4!

Loop iteration 3:
Desired: 3 Pods
Actual: 3 Pods (Pod 1, Pod 3, Pod 4)
â†’ No action (recovered!)
```

---

## 5ï¸âƒ£ Cloud Controller Manager (Optional)

### Vai TrÃ²: Cloud Integration

**Cloud Controller Manager = Liaison vá»›i cloud provider**

```
TÃ¡ch cloud-specific logic khá»i core K8s:
â”œâ”€â”€ Node lifecycle (táº¡o/xÃ³a VMs)
â”œâ”€â”€ LoadBalancer Services (provision cloud LB)
â”œâ”€â”€ Routes (configure VPC routing)
â””â”€â”€ Storage (provision cloud volumes)
```

### Cloud-Specific Controllers

**1. Node Controller (Cloud)**
```
Nhiá»‡m vá»¥:
â”œâ”€â”€ Create cloud VMs for new Nodes
â”œâ”€â”€ Delete cloud VMs when Nodes removed
â”œâ”€â”€ Update Node labels vá»›i cloud metadata
â””â”€â”€ Check Node existence in cloud

Example metadata:
labels:
  kubernetes.io/arch: amd64
  kubernetes.io/os: linux
  topology.kubernetes.io/region: us-central1
  topology.kubernetes.io/zone: us-central1-a
  node.kubernetes.io/instance-type: n1-standard-4
```

**2. Route Controller**
```
Nhiá»‡m vá»¥:
Configure cloud VPC routes for Pod network

Example (GCP):
Node 1: Pod CIDR 10.244.1.0/24
Node 2: Pod CIDR 10.244.2.0/24

Cloud Controller creates routes:
â”œâ”€â”€ 10.244.1.0/24 â†’ Node 1 VM
â””â”€â”€ 10.244.2.0/24 â†’ Node 2 VM

â†’ Pods can communicate cross-node!
```

**3. Service Controller**
```
Nhiá»‡m vá»¥:
Provision cloud LoadBalancers for Services

Example:
apiVersion: v1
kind: Service
metadata:
  name: webapp
spec:
  type: LoadBalancer  â† Cloud Controller acts!
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 8080

Cloud Controller:
1. Call cloud API: "Create LoadBalancer"
2. Get LoadBalancer IP
3. Update Service:
   status:
     loadBalancer:
       ingress:
       - ip: 35.xxx.xxx.xxx
```

---

## ğŸ”„ Component Interaction Example

### Complete Flow: Create Deployment

```
USER: kubectl create deployment nginx --image=nginx --replicas=3
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. API SERVER                               â”‚
â”‚    â€¢ Authenticate/Authorize                 â”‚
â”‚    â€¢ Validate Deployment spec               â”‚
â”‚    â€¢ Write to etcd                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. DEPLOYMENT CONTROLLER                    â”‚
â”‚    â€¢ Watch API Server                       â”‚
â”‚    â€¢ Detect new Deployment                  â”‚
â”‚    â€¢ Create ReplicaSet                      â”‚
â”‚    â€¢ â†’ API Server: "Create ReplicaSet"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. REPLICASET CONTROLLER                    â”‚
â”‚    â€¢ Watch API Server                       â”‚
â”‚    â€¢ Detect new ReplicaSet (replicas: 3)    â”‚
â”‚    â€¢ Create 3 Pods                          â”‚
â”‚    â€¢ â†’ API Server: "Create 3 Pods"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. API SERVER                               â”‚
â”‚    â€¢ Write Pods to etcd                     â”‚
â”‚    â€¢ Pods status: Pending (no Node)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. SCHEDULER                                â”‚
â”‚    â€¢ Watch for Pending Pods                 â”‚
â”‚    â€¢ Evaluate Nodes                         â”‚
â”‚    â€¢ Assign: Pod1â†’Node1, Pod2â†’Node2, Pod3â†’Node1 â”‚
â”‚    â€¢ â†’ API Server: Update Pod.spec.nodeNameâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. API SERVER                               â”‚
â”‚    â€¢ Update Pods in etcd                    â”‚
â”‚    â€¢ Pods now have Node assignments         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
      [Now kubelet takes over - see Worker Nodes section]
```

---

## ğŸ“ Kiá»ƒm Tra Hiá»ƒu Biáº¿t

### CÃ¢u Há»i Tá»± Kiá»ƒm Tra

**1. VÃ¬ sao API Server lÃ  component duy nháº¥t giao tiáº¿p vá»›i etcd?**
<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

**LÃ½ do:**
1. **Security**: Centralized access control
2. **Consistency**: Single source of truth
3. **Validation**: API Server validates táº¥t cáº£ writes
4. **Auditing**: Log má»i changes
5. **Simplicity**: Other components khÃ´ng cáº§n biáº¿t etcd details

Náº¿u má»i component Ä‘á»u access etcd:
- âŒ Security nightmare
- âŒ Validation bá»‹ bypass
- âŒ Inconsistent data
- âŒ Hard to audit
</details>

**2. Scheduler scoring: Node nÃ o Ä‘Æ°á»£c chá»n vÃ  táº¡i sao?**
```
Pod requests: CPU 1000m, RAM 2Gi
Nodes:
â”œâ”€â”€ Node A: Available 2 CPU, 6Gi RAM (30% used)
â”œâ”€â”€ Node B: Available 1.5 CPU, 3Gi RAM (50% used)
â””â”€â”€ Node C: Available 0.5 CPU, 1Gi RAM (80% used)
```

<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

**Filter phase:**
- Node A: âœ“ (Ä‘á»§ resources)
- Node B: âœ“ (Ä‘á»§ resources)
- Node C: âŒ (khÃ´ng Ä‘á»§ CPU)

**Scoring phase:**
- Node A: Score cao (nhiá»u available resources, balanced)
- Node B: Score trung bÃ¬nh (adequate resources)

**Winner: Node A**
LÃ½ do: Nhiá»u resources available nháº¥t, balanced CPU/RAM usage
</details>

**3. Controller nÃ o responsible cho viá»‡c gÃ¬?**

Match controllers vá»›i nhiá»‡m vá»¥:
1. Node Controller
2. ReplicaSet Controller
3. Endpoints Controller
4. Service Controller (Cloud)

Tasks:
a. Update Service Endpoints khi Pod IP changes
b. Maintain sá»‘ lÆ°á»£ng Pod replicas
c. Provision cloud LoadBalancer
d. Monitor Node health vÃ  evict Pods

<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

1. Node Controller â†’ d. Monitor Node health
2. ReplicaSet Controller â†’ b. Maintain Pod replicas
3. Endpoints Controller â†’ a. Update Service Endpoints
4. Service Controller (Cloud) â†’ c. Provision LoadBalancer
</details>

---

## ğŸ’ª BÃ i Táº­p Thá»±c HÃ nh

### BÃ i 1: Trace Complete Flow

**Scenario:** Deploy webapp, 1 Pod crashes sau 5 phÃºt

**Váº½ flow chi tiáº¿t tá»« lÃºc deploy Ä‘áº¿n lÃºc Pod recovered:**

<details>
<summary>Xem Ä‘Ã¡p Ã¡n</summary>

```
1. kubectl create deployment webapp --replicas=2
   â†“
2. API Server: Validate, write to etcd
   â†“
3. Deployment Controller: Create ReplicaSet
   â†“
4. ReplicaSet Controller: Create 2 Pods
   â†“
5. Scheduler: Assign Pods to Nodes
   â†“
6. kubelet: Start containers
   â†“
7. Pods Running! âœ…

--- 5 minutes later: Pod 1 crashes ---

8. kubelet detects crash
   â†“
9. kubelet â†’ API Server: "Pod 1 failed"
   â†“
10. API Server â†’ etcd: Update Pod 1 status
    â†“
11. ReplicaSet Controller watching:
    "Desired: 2, Actual: 1"
    â†“
12. ReplicaSet Controller â†’ API Server:
    "Create new Pod"
    â†“
13. Scheduler assigns new Pod to Node
    â†“
14. kubelet starts new Pod
    â†“
15. Recovered! 2 Pods Running âœ…
```
</details>

---

## ğŸ¯ Key Takeaways

### Ghi Nhá»› 5 Äiá»u Quan Trá»ng

1. **API Server = Central Hub**
   - Má»i interaction qua API Server
   - Giao tiáº¿p duy nháº¥t vá»›i etcd
   - Authentication, Authorization, Validation

2. **etcd = Single Source of Truth**
   - LÆ°u táº¥t cáº£ cluster state
   - Distributed, consistent
   - BACKUP REGULARLY!

3. **Scheduler = Smart Assignment**
   - Filter â†’ Score â†’ Bind
   - Consider resources, constraints, affinity
   - Optimize cluster utilization

4. **Controllers = Reconciliation**
   - Watch â†’ Compare â†’ Act
   - Desired state = Actual state
   - Self-healing automatic

5. **Watch Pattern Everywhere**
   - Components watch API Server
   - React to changes
   - Distributed event-driven architecture

---

## ğŸš€ Tiáº¿p Theo

Báº¡n Ä‘Ã£ hiá»ƒu chi tiáº¿t Control Plane!

**Next:** [2.3. Worker Nodes â†’](./03-worker-nodes.md)

á» pháº§n tiáº¿p theo, chÃºng ta sáº½ tÃ¬m hiá»ƒu Worker Nodes - nÆ¡i applications thá»±c sá»± cháº¡y.

---

[â¬…ï¸ 2.1. Overview](./01-overview.md) | [ğŸ  Má»¥c Lá»¥c ChÃ­nh](../README.md) | [ğŸ“‚ Pháº§n 2: Architecture](./README.md) | [â¡ï¸ 2.3. Worker Nodes](./03-worker-nodes.md)
